Sutton and Barto. (BTW, they recommend Bertsekas as another textbook
if this one is too easy; I'm probably good reading just one RL book).

# Ch1: Introduction

Main components are *policy*, *reward function*, *value function*, and
(optionally) a *model* of the environment.

They emphasize role of learning while interacting with the
environment. This lets you learn as you gain new experience. This lets
you learn against a player that changes strategy slowly!

They mention exploration vs exploitation. They mention temporal
difference learning of valuation functions and also
TD-Backgammon. They mention using other ML techniques in the valuation
function.

They mention that you can build in knowledge of the domain into RL
methods; presumably they will show how later. They mention partially
observed games, but say they won't deeply cover those.

They note that reinforcement learning is related to *optimal control*,
which has been studied for over a century.

They mention the *credit assignment problem*.

# Ch2: Bandit Problems

**I paused this book until later...**
