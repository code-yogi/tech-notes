These notes are supposed to go through Part III a second time for
better understanding.

## Ch13: Linear Factor Models

**PCA**

All these models are linear in the latent factors. They account for
any inability to fit a linear model with a noise.

Factor Analysis assumes a prior on the latent factors that is unit
normal. Noise is also normal, and independent: covariance matrix is
diagonal.

Probabilistic PCA just further assumes that the normal noise added has
constant variance. We recover PCA if the added normal noise is shrunk
to zero. It feels like whatever the additive noise, the MLE of the
linear transformation of `h` to `x` should always be the same, by
analogy to how with linear least squares regression, it doesn't matter
the variance of the additive noise. But I may not be entirely correct
about that. I am clearly a little confused on this point.

**Independent Component Analysis**

Used normally for separation of output into inputs. For instance, if
we have multiple speakers, or if we have an EEG and need to sort out
electrical signals from brain vs heart.

We again have a prior distribution on `h`, and then `x` is
deterministically generated by a linear transformation. Theoretically
we could learn via MLE, but a typical training goal is instead to try
to make the decomposed `h = W\inv x` as *independent* as possible.

It is required that the prior be non-Gaussian. If not, then the best
`W` is not identifiable, and you can mix the `h` around however you
want.

Many versions of ICA are "agnostic" of the choice of `p(h)`. Instead,
you try to learn a `W` such that the sample `W\inv x` is as
non-Gaussian as possible: for instance, by maximizing *kurtosis*. I
guess the idea of finding non-Gaussian signals makes sense to me:
Gaussian typically means mixture of factors.

They then talk about some nonlinear extensions. They also talk about a
version that has some kind of mixture modeling, where certain factors
are allowed to have statistical dependence. These include having
explicit groups which can have dependencies between factors in the
group (independent subspace analysis), or *topographic ICA*, which is
interesting. It assigns each factor a position in a space, and that
the closer two factors are in the space, the more dependence they are
allowed to have.

**Slow Feature Analysis**

Basic idea is to regularize a model of data that has a time dimension
to prefer features that change slowly over time.

Incidentally, you must require that features have variance equal to
one, else you can just "shrink" features to be basically zero. We also
need features `i` and `j` to have zero linear correlation (are
orthogonal) so that we learn different features rather than the same
one.

To learn non-linear features, we do quadratic basis expansion (add all
cross-terms). You can even do a deep version by repeating this process
iteratively.

SFA doesn't seem to give state-of-the-art performance. The idea is
that features don't necessarily have predictive value. Maybe we should
be trying to learn features that accurately predict the next
frame. Likewise, there may be a bias against predictive features that
change rapidly.

**Sparse Coding**

This is a slightly weird one. Again, it's a linear model with gaussian
noise added. The prior on `h` should be sparsifying, like the Laplace
distribution. Typically we assume a factorized distribution (`h_i` are
all independent).

Sparse coding with MLE is apparently intractable. This is presumably
similar to any situation with hidden latent variables. However, we can
use EM. We first pick the best codes `h` for the examples `x`. Then we
modify the weight matrix to maximize the probability of the observed
`x`s for the calculated `h` codes.

We don't typically try to learn a *parameteric encoder* that
calculates/estimates the best `h` for a given `x`. Instead, we do an
optimization procedure to step by step build a better `h`. This
optimization procedure doesn't have any parameters. The optimization
is the weighted sum of L1 error for `h`, and L2 reconstruction error
for `x` using the current `W` applied to `h`.

When going to use the model, this is in a sense an ideal encoding
strategy. For parameteric encoders, we have to worry about how well
they would encode an unseen `x` that may be quite different from the
observed `x` values in the training dataset. Instead, with our
non-parametric encoder we need only worry about the *decoder*
generalization.

However, it is slow because you have to run an optimization
algorithm. Also, the iterative algorithm means you can't backpropagate
through the encoder.

Because you assume a factorial distribution on `h`, you often generate
poor samples. This is similar to most other linear factor models. This
happens even when the you can reconstruct data well. That will drive
us toward considering deeper models that don't have a factorial
distribution on their deepest code layer.

## Ch14: Autoencoders

Consists of an encoder and a decoder. But I guess the idea is that
these could be deeper, or have a nonlinear mapping from `x` to `h` and
`h` to `x`.

There are deterministic encoders/decoders, and also stochastic ones. A
stochastic encoder would compute a set of parameters `\Theta = f(x)`
which gives a distribution over codes, while a stochastic decoder
would compute a set of parameters `\Theta = g(h)` to give a
distribution over `x`. Basically the two networks are learning
conditional inference. In sec14.4 they mention that to keep things
simpler, it is typical that the form of distribution on `x` is assumed
factorial. But sometimes you have mixture models at the end.

Autoencoders are typically used for dimensionality reduction and
feature learning.

**Undercomplete**

First strategy to consider is *undercomplete* autoencoders. They must
learn a latent representation that contains less bits than the input,
but can still reconstruct the input well. These should be the most
salient features of the input. If the decoder is constrained to be
linear and the loss is squared error, then the encoder is just
learning the PCA encoding.

By using deep models, we can generalize from PCA by allowing
non-linear encoding/decoding. That is, projection onto a non-linear
manifold.

However, if we have two much capacity, we may not encode useful
information in the latent variables. For instance, let's consider an
encoder where `h` is interpreted as an integer in binary
representation. If `h = 123`, then that means example number 123,
which the decoder has memorized. This would be a stupid
representation.

**Regularized**

Consider *overcomplete* codes; even a linear encoder can just copy all
the input information to the code trivially. Rather than limiting
encoder/decoder capacity, we will impose regularization to try to
ensure the code is useful.

**Sparse Autoencoders**

Sparse autoencoders imposes an L1 penalty on the codes. Now, unlike L2
regularization on weights, which can be interpreted as implying MAP
estimation with a gaussian prior on weights, this is a regularization
on *code*.

However, we can still interpret this as a prior *on codes*. In this
case, we are learning an MLE model, where there is a prior on codes,
*and* we are using just the single most probable estimate of `h` to
estimate the likelihood computed for `x`.

You can thus see the autoencoder as approximate training for a
generative model.

**Denoising Autoencoders**

We corrupt the input and then try to reconstruct to the original to
repair the damage. I believe this is basically saying that, as you
leave the manifold of high probability, the change in reconstruction
should be low.

They talk about *score matching*. This says that, equivalent to
learning `p(x)` is to learn `\grad_x log p(x)`. The relation to DAE is
this. The DAE learns a model where the score it assigns to `g(f(x))`
matches the score of the data distribution. That is, in directions
perpindicular to the manifold of high probability (directions in which
the score is zero), there should be no change in `g(f(x))`.

They talk a lot about DAEs in relation to RBMs, but I don't understand
this because I don't know RBMs well enough yet probably.

**Manifold Learning**

Early attempts were to, at each data point, impute a tangent plan
which spans the directions of variation associated with the differene
between an example and its nearest neighbors. This formed an
approximation of the manifold. The problem with this approach was that
it works for interpolation but not extrapolation.

**Contractive Autoencoders**

CAEs impose a regularization constrain on the encoder: the derivative
of `h = f(x)` with respect to `x` should be small. We usually compute
this via the Frobenius norm. It appears that when we are trying to do
representation learning, CAE tends to work better. Presumably that's
because you don't get any credit for resisting changes to `h` in the
decoder. That means maybe you learn a better representation `h`, since
the decoder may be sensitive to changes in `h`. (The paper by Rifai
seems to confirm my hypothesis).

The name comes from the idea that the encoder is trying to "contract"
the space to codes generated by the `x` values. Those directions with
high derivative tend to have semantic meaning.

A problem is that it can be hard to calculate the Jacobian in deeper
networks. Why is this particularly difficult?

One subtlety: if you let the decoder expand the space arbitrarily,
then the encoder can just shrink the space, learning nothing. So you
have to put some kind of constraint on the decoder's ability to
expand. The approach used by Rifai (maybe the one most interested in
CAEs) is to use transposed weights in the decoder. (In the paper, they
say that using the transpose weight matrix in the decoder is common,
but I don't know that I understand why. I mean, for a single linear
transformation maybe that makes sense? But even in a full-rank case,
the transpose is not the inverse...).

I've seen comments which suggest if `W` is the (transposed)
eigenvector matrix, then assuming no nonlinearity, this is just
PCA. And so optimization will learn `W`. But according to
(https://groups.google.com/forum/#!topic/theano-users/QilEmkFvDoE)
lots of people also use untied.

It looks like outside CAE, maybe people use tied weights because they
don't want you to use really small weights in the encoder, so that you
can exploit the linear activation function, and then use really large
weights in the decoder. But I'm getting into weeds maybe...

**Predictive Sparse Decomposition**

A mashup of sparse coding with nonparameteric encoding and parameteric
encoders. Your new loss function sums:

(1) L2 reconstruction loss.
(2) L1 loss of best `h` encoding found by iterative optimization.
(3) L2 loss of parameteric encoder `f(x)` to `h`.

You do EM. You find the best `h`, then you optimize `f` and `g`, then
do it again. One advantage is that `f(x)` is a very natural
initialization for `h` in your optimization algorithm.

When you go to do encoding online, it's now much cheaper thanks to
your parameteric encoder.

This is just a little diferent from learning a sparse coding model and
then training an encoder. The reason is because with PSD we can still
modify `g` to be compatible with a sparse coding that `f` does a good
job of predicting.

An advantage to PSD is that now you have a model which is fully
differentiable. That means it is a good choice for initialization of a
deep network.

**Semantic Hashing**

They mention that you can do semantic hashing if you can force the
autoencoder model to output ones and zeros. You could just round
logistic outputs, but they recommend explicitly training the model for
this purpose by slowly increasing an added gaussian noise to the
activations. That will make your encoder want to saturate the
sigmoids.

Now, when deploying the model for semantic hashing, you don't add the
noise, and just round to zero or one.

## Ch15: Representation Learning

The tension is between retaining information about the data and other
desirable properties, such as independence of variables.

Deep networks are often initialized with a stack of autoencoders or
RBMs. This is unsupervised greedy layer-wise pretraining. They claim
this isn't needed any more for densely connected networks, but not
why. This
(https://www.reddit.com/r/MachineLearning/comments/22u1yt/is_deep_learning_basically_just_neural_networks/cgqgy9w/)
claims that ReLU and dropout obsolesced pre-training, since (1) you
can propagate signal better and (2) dropout provides good
regularization. It is noticed that greedy pre-training doesn't seem to
be very helpful. But they do note that when you have large unlabeled
sets and small labeled sets, unsupervised pre-training can be helpful
still.

Aside: that thread has significant debate on whether sigmoids or ReLUs
are better. There is a claim that maybe sigmoids are better for
regression? Others say that ReLU isn't typically used for
autoencoders, and also that the benefit is really for deeper nets. I
don't feel like there is a clear explanation at the moment for me why
one would be better at some tasks and the other better at other
tasks...

For supervised tasks, sometimes this doesn't decrease training error,
but *does* reduce test error. That makes sense, because the network is
maybe biased toward weights that pass forward more information about
examples that aren't supervised-trained on. In this sense I think
Goodfellow considers it regularization.

There are many ways to do semi-supervised learning. One is virtual
adversarial, another is to try to minimize cost of a combined
unsupervised and supervised network. But the focus here is
pretraining.

They say there are two parts to the pretraining idea. First, that
pretraining has a "regularizing" effect (per above) and (to a lesser
extent), can improve optimization on the training set. The other part
of the question is: why does learning about the input distribution
help you solve the classification/regression problem? They admit this
isn't totally well understood.

It's not clear how the regularizing comes about. At first, people
thought it was because you were biased to certain local minima. But
experience shows that local minima aren't really a problem. My theory
would be that the starting weights bias you toward settings of the
weights that look equivalent, but work better for the test set. It's
not that this area is inaccessible, but that it has no attracting
power.

They mention that the unsupervised hopefully forces the data into a
representation that is useful to the supervised task. For instance,
hopefully the unsupervised codes linearly seperate the examples. But
it isn't well understood when this will happen. Therefore, the most
popular approach (I think), is to do the unsupervised and supervised
training simultaneously. I expect this means the supervised task will
tend to push the unsupervised task toward learning compatible codes.

Of course, the worse the original representation, the more useful is
unsupervised training. For this reason it is super-useful for words,
but less useful for images, since images do actually have more
semantic similarity embedded in L2 distance.

They suggest that the more complicated the supervised function, the
more attractive it is to regularize with unsupervised learning, versus
other weight decay type methods, which bias you toward simpler
functions which isn't really what you want.

Erhan explored unsupervised pre-training a fair bit. They found that
NNs always trained to the same region when pre-trained, which meant
there was less variance in outcome. That implies that there was less
overfitting. But why? There is a hypothesis that unsupervised training
encourages finding/use of the true causal factors.

They mention another disadvantage of two phases of
pretraining/training. You now have an outer loop to test pretraining
parameters, making everything slower. You also can't really choose how
you want to balance the effect of unsupervised loss versus supervised
loss: you either pre-train or not. So it seems even better to do both
simultaneously.

Experimentally, straight supervised training with dropout acheives
better performance on large and medium sized datasets. But then on
small datasets Bayesian approaches work better!

**Transfer Learning**

They then talk about transfer learning more generally. They mention
that a lot of times tasks share low-level features. Sometimes they
share *high* level features. For instance, speech-to-text might want
to keep the high layers, but swap out the low layers to be tuned for
different speakers.

They mention that empirically in transfer learing competitions (learn
on example set A features that are used for linear classification on
example set B), deeper representations seem to do a lot better with
far fewer examples in A.

They mention one-shot and zero-shot learning as extreme transfer
learning examples.

**What Makes A Representation Good?**

IT can be that if we learn the factors that cause `x`, then the `y` we
want to predict from `x` may itself be among those causal factors.

We normally try to impose sparsity or independence on the factors `h`;
that isn't the same as finding the causes. Why is that helpful? They
posit (without real proof) that once you identify causes, you can
generally separate these into independent factors. I don't know that I
totally believe that.

In general: if `y` is closely associated with the causal factors of
`x`, then representation learning ought to be helpful.

One challenge is that `y` may be associated with a cause of `x` that
might not otherwise seem to be among the most important causes. For
instance, imagine a task where you are asked: is there a house in the
background of the photo? Then if you use the L2 error for
reconstruction, this causal factor doesn't really impact the overall
image very much. In that case, it's not entirely clear if unsupervised
learning is going to be very helpful.

They note one solution idea: GANs. This allows us to learn what
details are salient and should be explained.

One advantage to learning causal factors is that if you learn `y | h`
(`h` is the causes of `x`), then this is robus to changes in the
distribution on `h`. This may be particularly important for transfer
learning, because when moving domains, the distribution on the
underlying causes of variation may change, but maybe the mechanics
stay the same. This is more likely to break if we learned things that
are *associated* with the causes.

They note that distributed representation is clearly important for
representation learning. This is what allows us to learn things about
cats and realize this can transfer to dogs. They give a geometric
intuition: that this lets us modify several different regions of space
by changing one weight.

I feel like this section is very hand-wavy and not saying much...

**Depth**

They claim that those factors that can be independent of each other
must be at a high level, because they interact at lower levels. Like
"roundness of face" and "big mouth" are pretty independent, but they
probably play out with complicated interactions at a lower level. That
suggests a need for *depth*.

**Asumptions That Help Us Find Underlying Causes**

If we're going to find true underlying causes, we need to have
assumptions that will bias us toward learning those causes. Otherwise,
we may learn features that are merely associated with the pheonomenon.

The most interesting of this laundry list (to me) is sparsity and
independence of high-level factors. The idea of sparsity is that most
features are not needed for most examples. For instance, a feature
about size of elephant trunk is not relevant to non-elephant
images. Likewise, independence of features. They also mention temporal
coherence, which relates to slow-feature analysis.

## Ch16: Structured Probabilistic Models for Deep Learning

Without structure, too many parameters. Sampling, density estimation,
conditional queries, et cetera are too hard. And statistical
efficiency is too low.

In a Bayes net, the number of parameters is potentially exponentional
in the number of parents (assuming binary variables). For Markov
network, it is exponentional in clique size.

Typical that we use an *energy function* where `p(x) =
exp(-E(x))`. This is a Boltzmann distribution; machines with this kind
of distribution can be called Boltzmann machines. Originally BMs had
only binary variables, but that isn't true anymore. BMs typically have
latent variables, otherwise we call these just Markov fields or even
just log-linear models.

Indeed, with this format, then you can just talk about each clique
setting having a certain energy, which is the log of its *clique
potential*.

By association with physics, the unnormalized marginal log probability
of `x` (that is, `log p\tilde(x)`) is sometimes called the *free
energy*. We can calculate this by summing out over all `h` `E(x, h)`.

It's not entirely clear to me when to prefer Bayes nets versus Markov
networks. Clearly, in a trivial sense, they are equivalent (clique of
entire graph), but I presume some probability distributions are more
naturally represented as Bayes nets and others with Markov networks.

**Sampling**

Ancestral sampling of Bayes nets is very efficient. If you need to
condition on something upstream and then sample on something
downstream, that is cool too. But if you need to sample from something
upstream, then you will no longer have a fast sampling approach.

Of course, you can do Gibbs sampling on undirected models, but you
have no real assurance of when this will mix.

An advantage to structured models is that we can inject our knowledge
by choosing a structure we think is appropriate. For instance,
consider LDA.

**Hidden Variables**

These can often be added and then relationships between the visible
units can be expressed more succinctly. And the hidden units now
become a useful encoding/representation that can be used for other
tasks.

**Inference**

Can be hard to calculate probabilities. Examples when we want to do
this: expected hidden value code for visible units, conditional
visible unit query (`p(y | x)`; `y` isn't hidden per se, because it is
observed in the training set), maximum likelihood training (because we
need `p(x)`).

Most models we use won't allow exact inference. But approximate
inference can be fast. Most common DL approximate inference technique
is variational inference.

**Deep Graphical Models**

They mention that the big distinction is that latent variables have no
explicitly intended purpose in Deep Models. We figure that the model
will learn that.

Deep models also have dense connectivity to latent variables. Thus
Loopy Belief Propagation, a typical approximation approach in sparse
networks, doesn't work well in these dense networks. So a big
difference from classic PGM is that we don't use LBP.

**Restricted Boltzmann Machine**

The quintessential deep undirected model. Single layer of hidden units
and of visible units. Only pairwise connections between hidden and
visible units. Dense connectivity. Designed to be easy to perform
Gibbs sampling on.

RBM has factorial conditional distributions `p(v | h)` and `p(h |
v)`. The first is `\prod_i p(v_i | h)` and the second is `\prod_i
p(h_i | v)`.

Likewise, `p(v_i = 1 | h)` is easy to compute. We take `\sigma(b_i +
W_{i, :} h)`.

This makes it very easy to do *block Gibbs sampling*. So sampling can
be fast!

It is easy to calculate the derivative of the energy function: if you
know both `v` and `h`. That means it is possible to do maximum
likelihood if everything were visible. But I assume we have an EM part
here, where we use the derivatives for optimization in the inner loop?

Not quite! Training uses the contrastive divergence concept. My
approach suggested won't work because having the derivative of the
energy function gives you the derivative of the unnormalized
probability of an example, but does *not* give you the derivative of
the normalized probability, since a change to the potentials affects
not only `p\tilde_\theta(x)` AND `Z(\theta)`.

## Ch17: Monte Carlo Methods

There are two reasons to sample. The first is sometimes we literally
want to produce samples: sample images, or fill in missing parts of an
image. The other is that we may use the samples to help approximate a
sum or integral. The more samples we draw, the better our estimate of
the sum/integral. A good example is if we want to compute the gradient
of the log partition function, which can be intractable.

**Importance Sampling**

There is a technique called *importance sampling* which can be
useful. Here we sample from a distribution `q(x)` and take the
empirical average of `p(x)/q(x) f(x)`. Of course, this will converge
to the right expectation of `f(x)` no matter the choice of `q`.

What is the best choice for `q`? Here we want the `q` that minimizes
the variance after `n` samples. The best choice is that `q*` (the best
`q`) choose `x` with probability proportional to `p(x) |f(x)|`. This
basically says: be the same as `p`, except be biased toward `x` that
have large magnitude. This is probably most easily understood if you
think of rare events that are massive (e.g., black swans).

Note that the best `q*` requires a normalizing constant `Z`. Typically
it is not practical to calculate `Z`. But we can often choose `q`
values that are better than just `p`.

**Biased Importance Sampling**

There is also *biased importance sampling*, which doesn't require
explicit normalization. Here you sample from `q\tilde`, and weight
`f(x)` as usual `p\tilde(x)/q\tilde(x)`. But then you "normalize" by
the sum of `p\tilde(x) / q\tilde(x)`, which approximates the
proper normalization constant.

This actually results in a biased estimate for finite `n`, but it is
*consistent*: it will converge to the right answer in the limit,
because then the normalization will approach the correct value.

**Sampling in High Dimensions Is Hard**

If `q` is a uniform distribution on a high-dimensional space, then it
is often that `q(x) >> p(x)|f(x)|`, which means you will sample
useless values that either don't actually occur very often, or are
very small in magnitude and don't contribute a lot for that reason. On
the other hand, for presumably fewer `x` values where `q(x) <<
p(x)|f(x)|`, you will have a situation where you are not sampling
important values that have big impact, but don't occur very often.

Unfortunately, when we have many dimensions the distribution can have
"high dynamic range," which makes the scenario described above
common. Still, importance sampling is used pretty often.

**MCMC**

The idea is that `p` may be hard to sample from, and that there is no
good `q` that will give you low-variance.

MCMC is best understood for discrete case first. Say you have a
distribution over `k` states. That's a vector in `R^k` that sums to
one. Now, consider a stochastic matrix `T`. Now, there's some proof
that a stochastic matrix always has a real, principal eigenvector with
eigenvalue equal to one. That means, that `T^k v` will approach this
eigenvector as `k \to \infinity`, no matter the initial choice of
`v`. This is the *stationary distribution* of the transition matrix
`T`. (So long as `v` is not orthogonal to the eigenvector, which is
probability zero under any reasonable initialization distribution).

The point is this: let's choose a transition matrix such that in the
limit it will converge to the right stationary distribution: the `p`
that is desired!

This shows the mixing, but if at each step you sample from the
calculated probability distribution, then in the limit your sample is
distributed as if by the principal eigenvector.

The same trick holds for continuous distributions.

After mixing (aka *burn in*), ever sample has probability `p(x)`, but
of course subsequent samples are not independent.

You can *thin*, by dropping samples until you feel like you have
wandered far enough awawy. Alternatively, you can have totally
uncorrelated samples if you use different chains. This could exploit
parallelism. But I presume mixing time is less than reasonable
decorrelation time, so overall CPU utilization would be higher.

Unfortunately we don't typically have very good tests for whether a
chain is mixed. Theoretically, if we had a discrete case, we could
examine the second eigenvector, and see if the eigenvalue is
small. However, if we have a number of variables, the number of states
is exponential in the number of variables, and the number of entries
in a stochastic matrix is that squared...

**Gibbs Sampling**

Nowhere have we said how we should construct the transition function
for our MCMC algorithm. We need to discuss that!

This says, pick a single value and update it based on its conditional
probability given the values of the other variables. This is typically
easy. This is *Gibbs sampling*.

*Block Gibbs sampling* is when you update many variables
simultaneously. You can do this when all the variables updated are
conditionally independent given their neighbors. This is for instance
*exactly* the case when dealing with the RBM.

There are other possible MCMC approaches like Metropolis-Hastings. I
forget how that works. But they say that Gibbs is much more widely
used in DL than MH.

**Sampling Between Modes**

The problem is that it can take a long time to wander from one area of
high probability to another if there are *very* improbable spaces in
between. This in particular happens when there are strong dependencies
between variables.

They show an example of a sharp, very skewed normal. It's hard to walk
from one side to the other, because the steps you can make are very
small.

All else equal, if you can sample highly dependent variables in a
block, then that is ideal. So, for instance, I expect blocked Gibbs
for RBM is probably way better than pure Gibbs.

All the same, they show a problem with MNIST, where it's really hard
to move from one class to another. The reason is that the `h` code is
still very highly dependent on the `v`, and vice versa.

The fundamental problem is: we want `x` to be highly dependent on `h`:
that means `h` is retaining the info for `x`. But the more we succeed
at this, the harder it will be to wander.

In particular, they note that MCMC is going to have a hard time
especially when the manifolds for different classes are
disconnected. Then it's going to be really hard to wander. But that is
really unfortunate because that's exactly the case we typically
expect!

**Tempering**

Here, we throw in a temperature parameter. That is, we scale the
energy by `\beta`: `p(x) ~ \exp(-\beta E(x))`. The higher `\beta` is,
the "colder" the environment is, and the probability distributino
becomes more sharply peaked. For small (positive) `\beta`, the higher
the temperature, and the more uniform.

By changing the temperature, we might be able to help ourselves move
between modes. However, they mention that this approach has not yet
shown a lot of fruit, because we have to be very careful how we change
the temperature.

**Depth**

There is a note, which I think they just add for interest, that deeper
representations seem to be more unimodal than shallow
representations. Experimentally, I think they trained a deep
autoencoder representation, and then used this for an RBM and the RBM
mixed better.

However, they note that it isn't clear how to use this insight
practically. So they seem to include it just to help you build
intuition.

## Ch18: Confronting the Partition Function

We talked about how to sample. But now we need to talk about how to
optimize a model with a partition function. Returning to the RBM
example: we praticularly want to know the gradient of `Z` as a
function of `\theta`. We need that to do gradient descent: we can
easily calculate the derivative of `E(v, h)`, but that doesn't tell us
how a change to `\theta` affects `Z`. If `E(v, h)` goes down a lot,
but `Z` goes down even more, then that means that `p(v, h)` is *less*
probable.

Everything makes sense in the log probability space. We have:

    \log p(v, h; \theta) = \log (p\tilde(v, h; \theta) / Z(\theta))
    = \log p\tilde(v, h; \theta) - \log Z(\theta)

This decomposition consists of a *positive* "phase" (the term where
you increase the unnormalized probability of `v, h`) and a *negative*
"phase" (the term where you try to decrease the partition function
`Z`. Effectively: decrease the unnormalized probability of all other
configurations. When these forces are equal, you've got MLE.

Because the partition function sums out/integrates over all variables,
it is typically the more difficult quantity to calculate. The positive
term is the easy part.

Now, we can show that the gradient of `\log Z(\theta)` is equal to the
expectation of `\grad_\theta \log p\tilde(x)` for `x ~ p(x)` according
to the model. That's actually sort of interesting: the gradient of
`Z(\theta)` would be the *sum* of the gradients, not the
expectation. But it is also logical: you are comparing what you are
doing on the positive example to the change you make at a "typical"
example.

This is basically the idea: we want to (1) push down on the energy at
training set datapoints, and (2) pull up on the energy everywhere, in
particular proportional to the probability that the model assigns to
those datapoints.

This is basically saying: model, correct yourself by making your
incorrect beliefs less likely, and your correct beliefs more likely.

**Naive MCMC**

Because an expectation is involved, this is a classic time to do an
MCMC thing!

What you do is this. Take a mini-batch. Calculuate the positive
gradient.

Next, generate a sample by mixing an MCMC chain. Find the gradient
with respect to `\theta` of the *hallucinations*. Now subtract the
positive and negative gradients and use it to update.

The problem is that you're burning in an MCMC chain in the inner loop,
which is very slow. Possibly ~100 MCMC steps will be needed for a
small image patch.

**Contrastive Divergence**

The idea is that you can avoid long burn-in time if you start the
chain out in a distribution that is approximately the target
distribution.

The model `p(x)` is supposed to converge to the true, target
distribution. So you can start your MCMC by sampling a random example
from the training dataset. This is sampling from the target
distribution, right?

Now, you need a sample from *your* distribution. So you still have to
do MCMC, using maybe 1-20 Gibbs sampling steps. By using a reasonable
starting point, rather than randomly sampling an initial point,
burn in/mixing requires far fewer steps.

It is typicaly to just use your mini-batch and do a number of Gibbs
sampling steps.

They note that in the beginning, when your model distribution is very
far from the true distribution, you probably won't properly mix. While
you may push down on the probability in the wrong places, you will
still pull up in the right places. As you get closer to the true
distribution, the initialization from the dataset should be more of a
good starting point and the chain will mix better.

Indeed, this is the traditional way to do RBM training. You take an
example `v`, and sample `h`. You use `v, h` to calculate your positive
gradient. Then you resample `v'`, and then `h'` too. Use `v', h` for
your negative gradient calculation.

**TODO**: Was up to spurious modes.
