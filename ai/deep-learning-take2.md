These notes are supposed to go through Part III a second time for
better understanding.

## Ch13: Linear Factor Models

**PCA**

All these models are linear in the latent factors. They account for
any inability to fit a linear model with a noise.

Factor Analysis assumes a prior on the latent factors that is unit
normal. Noise is also normal, and independent: covariance matrix is
diagonal.

Probabilistic PCA just further assumes that the normal noise added has
constant variance. We recover PCA if the added normal noise is shrunk
to zero. It feels like whatever the additive noise, the MLE of the
linear transformation of `h` to `x` should always be the same, by
analogy to how with linear least squares regression, it doesn't matter
the variance of the additive noise. But I may not be entirely correct
about that. I am clearly a little confused on this point.

**Independent Component Analysis**

Used normally for separation of output into inputs. For instance, if
we have multiple speakers, or if we have an EEG and need to sort out
electrical signals from brain vs heart.

We again have a prior distribution on `h`, and then `x` is
deterministically generated by a linear transformation. Theoretically
we could learn via MLE, but a typical training goal is instead to try
to make the decomposed `h = W\inv x` as *independent* as possible.

It is required that the prior be non-Gaussian. If not, then the best
`W` is not identifiable, and you can mix the `h` around however you
want.

Many versions of ICA are "agnostic" of the choice of `p(h)`. Instead,
you try to learn a `W` such that the sample `W\inv x` is as
non-Gaussian as possible: for instance, by maximizing *kurtosis*. I
guess the idea of finding non-Gaussian signals makes sense to me:
Gaussian typically means mixture of factors.

They then talk about some nonlinear extensions. They also talk about a
version that has some kind of mixture modeling, where certain factors
are allowed to have statistical dependence. These include having
explicit groups which can have dependencies between factors in the
group (independent subspace analysis), or *topographic ICA*, which is
interesting. It assigns each factor a position in a space, and that
the closer two factors are in the space, the more dependence they are
allowed to have.

**Slow Feature Analysis**

Basic idea is to regularize a model of data that has a time dimension
to prefer features that change slowly over time.

Incidentally, you must require that features have variance equal to
one, else you can just "shrink" features to be basically zero. We also
need features `i` and `j` to have zero linear correlation (are
orthogonal) so that we learn different features rather than the same
one.

To learn non-linear features, we do quadratic basis expansion (add all
cross-terms). You can even do a deep version by repeating this process
iteratively.

SFA doesn't seem to give state-of-the-art performance. The idea is
that features don't necessarily have predictive value. Maybe we should
be trying to learn features that accurately predict the next
frame. Likewise, there may be a bias against predictive features that
change rapidly.

**Sparse Coding**

This is a slightly weird one. Again, it's a linear model with gaussian
noise added. The prior on `h` should be sparsifying, like the Laplace
distribution. Typically we assume a factorized distribution (`h_i` are
all independent).

Sparse coding with MLE is apparently intractable. This is presumably
similar to any situation with hidden latent variables. However, we can
use EM. We first pick the best codes `h` for the examples `x`. Then we
modify the weight matrix to maximize the probability of the observed
`x`s for the calculated `h` codes.

We don't typically try to learn a *parameteric encoder* that
calculates/estimates the best `h` for a given `x`. Instead, we do an
optimization procedure to step by step build a better `h`. This
optimization procedure doesn't have any parameters. The optimization
is the weighted sum of L1 error for `h`, and L2 reconstruction error
for `x` using the current `W` applied to `h`.

When going to use the model, this is in a sense an ideal encoding
strategy. For parameteric encoders, we have to worry about how well
they would encode an unseen `x` that may be quite different from the
observed `x` values in the training dataset. Instead, with our
non-parametric encoder we need only worry about the *decoder*
generalization.

However, it is slow because you have to run an optimization
algorithm. Also, the iterative algorithm means you can't backpropagate
through the encoder.

Because you assume a factorial distribution on `h`, you often generate
poor samples. This is similar to most other linear factor models. This
happens even when the you can reconstruct data well. That will drive
us toward considering deeper models that don't have a factorial
distribution on their deepest code layer.

## Ch14: Autoencoders

Consists of an encoder and a decoder. But I guess the idea is that
these could be deeper, or have a nonlinear mapping from `x` to `h` and
`h` to `x`.

There are deterministic encoders/decoders, and also stochastic ones. A
stochastic encoder would compute a set of parameters `\Theta = f(x)`
which gives a distribution over codes, while a stochastic decoder
would compute a set of parameters `\Theta = g(h)` to give a
distribution over `x`. Basically the two networks are learning
conditional inference. In sec14.4 they mention that to keep things
simpler, it is typical that the form of distribution on `x` is assumed
factorial. But sometimes you have mixture models at the end.

Autoencoders are typically used for dimensionality reduction and
feature learning.

**Undercomplete**

First strategy to consider is *undercomplete* autoencoders. They must
learn a latent representation that contains less bits than the input,
but can still reconstruct the input well. These should be the most
salient features of the input. If the decoder is constrained to be
linear and the loss is squared error, then the encoder is just
learning the PCA encoding.

By using deep models, we can generalize from PCA by allowing
non-linear encoding/decoding. That is, projection onto a non-linear
manifold.

However, if we have two much capacity, we may not encode useful
information in the latent variables. For instance, let's consider an
encoder where `h` is interpreted as an integer in binary
representation. If `h = 123`, then that means example number 123,
which the decoder has memorized. This would be a stupid
representation.

**Regularized**

Consider *overcomplete* codes; even a linear encoder can just copy all
the input information to the code trivially. Rather than limiting
encoder/decoder capacity, we will impose regularization to try to
ensure the code is useful.

**Sparse Autoencoders**

Sparse autoencoders imposes an L1 penalty on the codes. Now, unlike L2
regularization on weights, which can be interpreted as implying MAP
estimation with a gaussian prior on weights, this is a regularization
on *code*.

However, we can still interpret this as a prior *on codes*. In this
case, we are learning an MLE model, where there is a prior on codes,
*and* we are using just the single most probable estimate of `h` to
estimate the likelihood computed for `x`.

You can thus see the autoencoder as approximate training for a
generative model.

**Denoising Autoencoders**

We corrupt the input and then try to reconstruct to the original to
repair the damage. I believe this is basically saying that, as you
leave the manifold of high probability, the change in reconstruction
should be low.

They talk about *score matching*. This says that, equivalent to
learning `p(x)` is to learn `\grad_x log p(x)`. The relation to DAE is
this. The DAE learns a model where the score it assigns to `g(f(x))`
matches the score of the data distribution. That is, in directions
perpindicular to the manifold of high probability (directions in which
the score is zero), there should be no change in `g(f(x))`.

They talk a lot about DAEs in relation to RBMs, but I don't understand
this because I don't know RBMs well enough yet probably.

**Manifold Learning**

Early attempts were to, at each data point, impute a tangent plan
which spans the directions of variation associated with the differene
between an example and its nearest neighbors. This formed an
approximation of the manifold. The problem with this approach was that
it works for interpolation but not extrapolation.

**Contractive Autoencoders**

CAEs impose a regularization constrain on the encoder: the derivative
of `h = f(x)` with respect to `x` should be small. We usually compute
this via the Frobenius norm. It appears that when we are trying to do
representation learning, CAE tends to work better. Presumably that's
because you don't get any credit for resisting changes to `h` in the
decoder. That means maybe you learn a better representation `h`, since
the decoder may be sensitive to changes in `h`.

The name comes from the idea that the encoder is trying to "contract"
the space to codes generated by the `x` values. Those directions with
high derivative tend to have semantic meaning.

A problem is that it can be hard to calculate the Jacobian in deeper
networks. Why is this particularly difficult?

One subtlety: if you let the decoder expand the space arbitrarily,
then the encoder can just shrink the space, learning nothing. So you
have to put some kind of constraint on the decoder's ability to
expand. The approach used by Rifai (maybe the one most interested in
CAEs) is to use transposed weights in the decoder?

**Predictive Sparse Decomposition**

A mashup of sparse coding with nonparameteric encoding and parameteric
encoders. Your new loss function sums:

(1) L2 reconstruction loss.
(2) L1 loss of best `h` encoding found by iterative optimization.
(3) L2 loss of parameteric encoder `f(x)` to `h`.

You do EM. You find the best `h`, then you optimize `f` and `g`, then
do it again. One advantage is that `f(x)` is a very natural
initialization for `h` in your optimization algorithm.

When you go to do encoding online, it's now much cheaper thanks to
your parameteric encoder.

This is just a little diferent from learning a sparse coding model and
then training an encoder. The reason is because with PSD we can still
modify `g` to be compatible with a sparse coding that `f` does a good
job of predicting.

An advantage to PSD is that now you have a model which is fully
differentiable. That means it is a good choice for initialization of a
deep network.
