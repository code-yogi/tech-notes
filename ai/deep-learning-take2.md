These notes are supposed to go through Part III a second time for
better understanding.

## Ch13: Linear Factor Models

**PCA**

All these models are linear in the latent factors. They account for
any inability to fit a linear model with a noise.

Factor Analysis assumes a prior on the latent factors that is unit
normal. Noise is also normal, and independent: covariance matrix is
diagonal.

Probabilistic PCA just further assumes that the normal noise added has
constant variance. We recover PCA if the added normal noise is shrunk
to zero. It feels like whatever the additive noise, the MLE of the
linear transformation of `h` to `x` should always be the same, by
analogy to how with linear least squares regression, it doesn't matter
the variance of the additive noise. But I may not be entirely correct
about that. I am clearly a little confused on this point.

**Independent Component Analysis**

Used normally for separation of output into inputs. For instance, if
we have multiple speakers, or if we have an EEG and need to sort out
electrical signals from brain vs heart.

We again have a prior distribution on `h`, and then `x` is
deterministically generated by a linear transformation. Theoretically
we could learn via MLE, but a typical training goal is instead to try
to make the decomposed `h = W\inv x` as *independent* as possible.

It is required that the prior be non-Gaussian. If not, then the best
`W` is not identifiable, and you can mix the `h` around however you
want.

Many versions of ICA are "agnostic" of the choice of `p(h)`. Instead,
you try to learn a `W` such that the sample `W\inv x` is as
non-Gaussian as possible: for instance, by maximizing *kurtosis*. I
guess the idea of finding non-Gaussian signals makes sense to me:
Gaussian typically means mixture of factors.

They then talk about some nonlinear extensions. They also talk about a
version that has some kind of mixture modeling, where certain factors
are allowed to have statistical dependence. These include having
explicit groups which can have dependencies between factors in the
group (independent subspace analysis), or *topographic ICA*, which is
interesting. It assigns each factor a position in a space, and that
the closer two factors are in the space, the more dependence they are
allowed to have.

**Slow Feature Analysis**

Basic idea is to regularize a model of data that has a time dimension
to prefer features that change slowly over time.

Incidentally, you must require that features have variance equal to
one, else you can just "shrink" features to be basically zero. We also
need features `i` and `j` to have zero linear correlation (are
orthogonal) so that we learn different features rather than the same
one.

To learn non-linear features, we do quadratic basis expansion (add all
cross-terms). You can even do a deep version by repeating this process
iteratively.

SFA doesn't seem to give state-of-the-art performance. The idea is
that features don't necessarily have predictive value. Maybe we should
be trying to learn features that accurately predict the next
frame. Likewise, there may be a bias against predictive features that
change rapidly.

**Sparse Coding**

This is a slightly weird one. Again, it's a linear model with gaussian
noise added. The prior on `h` should be sparsifying, like the Laplace
distribution. Typically we assume a factorized distribution (`h_i` are
all independent).

Sparse coding with MLE is apparently intractable. This is presumably
similar to any situation with hidden latent variables. However, we can
use EM. We first pick the best codes `h` for the examples `x`. Then we
modify the weight matrix to maximize the probability of the observed
`x`s for the calculated `h` codes.

We don't typically try to learn a *parameteric encoder* that
calculates/estimates the best `h` for a given `x`. Instead, we do an
optimization procedure to step by step build a better `h`. This
optimization procedure doesn't have any parameters. The optimization
is the weighted sum of L1 error for `h`, and L2 reconstruction error
for `x` using the current `W` applied to `h`.

When going to use the model, this is in a sense an ideal encoding
strategy. For parameteric encoders, we have to worry about how well
they would encode an unseen `x` that may be quite different from the
observed `x` values in the training dataset. Instead, with our
non-parametric encoder we need only worry about the *decoder*
generalization.

However, it is slow because you have to run an optimization
algorithm. Also, the iterative algorithm means you can't backpropagate
through the encoder.

Because you assume a factorial distribution on `h`, you often generate
poor samples. This is similar to most other linear factor models. This
happens even when the you can reconstruct data well. That will drive
us toward considering deeper models that don't have a factorial
distribution on their deepest code layer.

## Ch14: Autoencoders

Consists of an encoder and a decoder. But I guess the idea is that
these could be deeper, or have a nonlinear mapping from `x` to `h` and
`h` to `x`.

There are deterministic encoders/decoders, and also stochastic ones. A
stochastic encoder would compute a set of parameters `\Theta = f(x)`
which gives a distribution over codes, while a stochastic decoder
would compute a set of parameters `\Theta = g(h)` to give a
distribution over `x`. Basically the two networks are learning
conditional inference. In sec14.4 they mention that to keep things
simpler, it is typical that the form of distribution on `x` is assumed
factorial. But sometimes you have mixture models at the end.

Autoencoders are typically used for dimensionality reduction and
feature learning.

**Undercomplete**

First strategy to consider is *undercomplete* autoencoders. They must
learn a latent representation that contains less bits than the input,
but can still reconstruct the input well. These should be the most
salient features of the input. If the decoder is constrained to be
linear and the loss is squared error, then the encoder is just
learning the PCA encoding.

By using deep models, we can generalize from PCA by allowing
non-linear encoding/decoding. That is, projection onto a non-linear
manifold.

However, if we have two much capacity, we may not encode useful
information in the latent variables. For instance, let's consider an
encoder where `h` is interpreted as an integer in binary
representation. If `h = 123`, then that means example number 123,
which the decoder has memorized. This would be a stupid
representation.

**Regularized**

Consider *overcomplete* codes; even a linear encoder can just copy all
the input information to the code trivially. Rather than limiting
encoder/decoder capacity, we will impose regularization to try to
ensure the code is useful.

**Sparse Autoencoders**

Sparse autoencoders imposes an L1 penalty on the codes. Now, unlike L2
regularization on weights, which can be interpreted as implying MAP
estimation with a gaussian prior on weights, this is a regularization
on *code*.

However, we can still interpret this as a prior *on codes*. In this
case, we are learning an MLE model, where there is a prior on codes,
*and* we are using just the single most probable estimate of `h` to
estimate the likelihood computed for `x`.

You can thus see the autoencoder as approximate training for a
generative model.

**Denoising Autoencoders**

We corrupt the input and then try to reconstruct to the original to
repair the damage. I believe this is basically saying that, as you
leave the manifold of high probability, the change in reconstruction
should be low.

They talk about *score matching*. This says that, equivalent to
learning `p(x)` is to learn `\grad_x log p(x)`. The relation to DAE is
this. The DAE learns a model where the score it assigns to `g(f(x))`
matches the score of the data distribution. That is, in directions
perpindicular to the manifold of high probability (directions in which
the score is zero), there should be no change in `g(f(x))`.

They talk a lot about DAEs in relation to RBMs, but I don't understand
this because I don't know RBMs well enough yet probably.

**Manifold Learning**

Early attempts were to, at each data point, impute a tangent plan
which spans the directions of variation associated with the differene
between an example and its nearest neighbors. This formed an
approximation of the manifold. The problem with this approach was that
it works for interpolation but not extrapolation.

**Contractive Autoencoders**

CAEs impose a regularization constrain on the encoder: the derivative
of `h = f(x)` with respect to `x` should be small. We usually compute
this via the Frobenius norm. It appears that when we are trying to do
representation learning, CAE tends to work better. Presumably that's
because you don't get any credit for resisting changes to `h` in the
decoder. That means maybe you learn a better representation `h`, since
the decoder may be sensitive to changes in `h`. (The paper by Rifai
seems to confirm my hypothesis).

The name comes from the idea that the encoder is trying to "contract"
the space to codes generated by the `x` values. Those directions with
high derivative tend to have semantic meaning.

A problem is that it can be hard to calculate the Jacobian in deeper
networks. Why is this particularly difficult?

One subtlety: if you let the decoder expand the space arbitrarily,
then the encoder can just shrink the space, learning nothing. So you
have to put some kind of constraint on the decoder's ability to
expand. The approach used by Rifai (maybe the one most interested in
CAEs) is to use transposed weights in the decoder. (In the paper, they
say that using the transpose weight matrix in the decoder is common,
but I don't know that I understand why. I mean, for a single linear
transformation maybe that makes sense? But even in a full-rank case,
the transpose is not the inverse...).

I've seen comments which suggest if `W` is the (transposed)
eigenvector matrix, then assuming no nonlinearity, this is just
PCA. And so optimization will learn `W`. But according to
(https://groups.google.com/forum/#!topic/theano-users/QilEmkFvDoE)
lots of people also use untied.

It looks like outside CAE, maybe people use tied weights because they
don't want you to use really small weights in the encoder, so that you
can exploit the linear activation function, and then use really large
weights in the decoder. But I'm getting into weeds maybe...

**Predictive Sparse Decomposition**

A mashup of sparse coding with nonparameteric encoding and parameteric
encoders. Your new loss function sums:

(1) L2 reconstruction loss.
(2) L1 loss of best `h` encoding found by iterative optimization.
(3) L2 loss of parameteric encoder `f(x)` to `h`.

You do EM. You find the best `h`, then you optimize `f` and `g`, then
do it again. One advantage is that `f(x)` is a very natural
initialization for `h` in your optimization algorithm.

When you go to do encoding online, it's now much cheaper thanks to
your parameteric encoder.

This is just a little diferent from learning a sparse coding model and
then training an encoder. The reason is because with PSD we can still
modify `g` to be compatible with a sparse coding that `f` does a good
job of predicting.

An advantage to PSD is that now you have a model which is fully
differentiable. That means it is a good choice for initialization of a
deep network.

**Semantic Hashing**

They mention that you can do semantic hashing if you can force the
autoencoder model to output ones and zeros. You could just round
logistic outputs, but they recommend explicitly training the model for
this purpose by slowly increasing an added gaussian noise to the
activations. That will make your encoder want to saturate the
sigmoids.

Now, when deploying the model for semantic hashing, you don't add the
noise, and just round to zero or one.

## Ch15: Representation Learning

The tension is between retaining information about the data and other
desirable properties, such as independence of variables.

Deep networks are often initialized with a stack of autoencoders or
RBMs. This is unsupervised greedy layer-wise pretraining. They claim
this isn't needed any more for densely connected networks, but not
why. This
(https://www.reddit.com/r/MachineLearning/comments/22u1yt/is_deep_learning_basically_just_neural_networks/cgqgy9w/)
claims that ReLU and dropout obsolesced pre-training, since (1) you
can propagate signal better and (2) dropout provides good
regularization. It is noticed that greedy pre-training doesn't seem to
be very helpful. But they do note that when you have large unlabeled
sets and small labeled sets, unsupervised pre-training can be helpful
still.

Aside: that thread has significant debate on whether sigmoids or ReLUs
are better. There is a claim that maybe sigmoids are better for
regression? Others say that ReLU isn't typically used for
autoencoders, and also that the benefit is really for deeper nets. I
don't feel like there is a clear explanation at the moment for me why
one would be better at some tasks and the other better at other
tasks...

For supervised tasks, sometimes this doesn't decrease training error,
but *does* reduce test error. That makes sense, because the network is
maybe biased toward weights that pass forward more information about
examples that aren't supervised-trained on. In this sense I think
Goodfellow considers it regularization.

There are many ways to do semi-supervised learning. One is virtual
adversarial, another is to try to minimize cost of a combined
unsupervised and supervised network. But the focus here is
pretraining.

They say there are two parts to the pretraining idea. First, that
pretraining has a "regularizing" effect (per above) and (to a lesser
extent), can improve optimization on the training set. The other part
of the question is: why does learning about the input distribution
help you solve the classification/regression problem? They admit this
isn't totally well understood.

It's not clear how the regularizing comes about. At first, people
thought it was because you were biased to certain local minima. But
experience shows that local minima aren't really a problem. My theory
would be that the starting weights bias you toward settings of the
weights that look equivalent, but work better for the test set. It's
not that this area is inaccessible, but that it has no attracting
power.

They mention that the unsupervised hopefully forces the data into a
representation that is useful to the supervised task. For instance,
hopefully the unsupervised codes linearly seperate the examples. But
it isn't well understood when this will happen. Therefore, the most
popular approach (I think), is to do the unsupervised and supervised
training simultaneously. I expect this means the supervised task will
tend to push the unsupervised task toward learning compatible codes.

Of course, the worse the original representation, the more useful is
unsupervised training. For this reason it is super-useful for words,
but less useful for images, since images do actually have more
semantic similarity embedded in L2 distance.

They suggest that the more complicated the supervised function, the
more attractive it is to regularize with unsupervised learning, versus
other weight decay type methods, which bias you toward simpler
functions which isn't really what you want.

Erhan explored unsupervised pre-training a fair bit. They found that
NNs always trained to the same region when pre-trained, which meant
there was less variance in outcome. That implies that there was less
overfitting. But why? There is a hypothesis that unsupervised training
encourages finding/use of the true causal factors.

They mention another disadvantage of two phases of
pretraining/training. You now have an outer loop to test pretraining
parameters, making everything slower. You also can't really choose how
you want to balance the effect of unsupervised loss versus supervised
loss: you either pre-train or not. So it seems even better to do both
simultaneously.

Experimentally, straight supervised training with dropout acheives
better performance on large and medium sized datasets. But then on
small datasets Bayesian approaches work better!

**Transfer Learning**

They then talk about transfer learning more generally. They mention
that a lot of times tasks share low-level features. Sometimes they
share *high* level features. For instance, speech-to-text might want
to keep the high layers, but swap out the low layers to be tuned for
different speakers.

They mention that empirically in transfer learing competitions (learn
on example set A features that are used for linear classification on
example set B), deeper representations seem to do a lot better with
far fewer examples in A.

They mention one-shot and zero-shot learning as extreme transfer
learning examples.

**What Makes A Representation Good?**

IT can be that if we learn the factors that cause `x`, then the `y` we
want to predict from `x` may itself be among those causal factors.

We normally try to impose sparsity or independence on the factors `h`;
that isn't the same as finding the causes. Why is that helpful? They
posit (without real proof) that once you identify causes, you can
generally separate these into independent factors. I don't know that I
totally believe that.

In general: if `y` is closely associated with the causal factors of
`x`, then representation learning ought to be helpful.

One challenge is that `y` may be associated with a cause of `x` that
might not otherwise seem to be among the most important causes. For
instance, imagine a task where you are asked: is there a house in the
background of the photo? Then if you use the L2 error for
reconstruction, this causal factor doesn't really impact the overall
image very much. In that case, it's not entirely clear if unsupervised
learning is going to be very helpful.

They note one solution idea: GANs. This allows us to learn what
details are salient and should be explained.

One advantage to learning causal factors is that if you learn `y | h`
(`h` is the causes of `x`), then this is robus to changes in the
distribution on `h`. This may be particularly important for transfer
learning, because when moving domains, the distribution on the
underlying causes of variation may change, but maybe the mechanics
stay the same. This is more likely to break if we learned things that
are *associated* with the causes.

They note that distributed representation is clearly important for
representation learning. This is what allows us to learn things about
cats and realize this can transfer to dogs. They give a geometric
intuition: that this lets us modify several different regions of space
by changing one weight.

I feel like this section is very hand-wavy and not saying much...

**Depth**

They claim that those factors that can be independent of each other
must be at a high level, because they interact at lower levels. Like
"roundness of face" and "big mouth" are pretty independent, but they
probably play out with complicated interactions at a lower level. That
suggests a need for *depth*.

**Asumptions That Help Us Find Underlying Causes**

If we're going to find true underlying causes, we need to have
assumptions that will bias us toward learning those causes. Otherwise,
we may learn features that are merely associated with the pheonomenon.

The most interesting of this laundry list (to me) is sparsity and
independence of high-level factors. The idea of sparsity is that most
features are not needed for most examples. For instance, a feature
about size of elephant trunk is not relevant to non-elephant
images. Likewise, independence of features. They also mention temporal
coherence, which relates to slow-feature analysis.

## Ch16: Structured Probabilistic Models for Deep Learning

Without structure, too many parameters. Sampling, density estimation,
conditional queries, et cetera are too hard. And statistical
efficiency is too low.

In a Bayes net, the number of parameters is potentially exponentional
in the number of parents (assuming binary variables). For Markov
network, it is exponentional in clique size.

Typical that we use an *energy function* where `p(x) =
exp(-E(x))`. This is a Boltzmann distribution; machines with this kind
of distribution can be called Boltzmann machines. Originally BMs had
only binary variables, but that isn't true anymore. BMs typically have
latent variables, otherwise we call these just Markov fields or even
just log-linear models.

Indeed, with this format, then you can just talk about each clique
setting having a certain energy, which is the log of its *clique
potential*.

By association with physics, the unnormalized marginal log probability
of `x` (that is, `log p\tilde(x)`) is sometimes called the *free
energy*. We can calculate this by summing out over all `h` `E(x, h)`.

It's not entirely clear to me when to prefer Bayes nets versus Markov
networks. Clearly, in a trivial sense, they are equivalent (clique of
entire graph), but I presume some probability distributions are more
naturally represented as Bayes nets and others with Markov networks.

**Sampling**

Ancestral sampling of Bayes nets is very efficient. If you need to
condition on something upstream and then sample on something
downstream, that is cool too. But if you need to sample from something
upstream, then you will no longer have a fast sampling approach.

Of course, you can do Gibbs sampling on undirected models, but you
have no real assurance of when this will mix.

An advantage to structured models is that we can inject our knowledge
by choosing a structure we think is appropriate. For instance,
consider LDA.

**Hidden Variables**

These can often be added and then relationships between the visible
units can be expressed more succinctly. And the hidden units now
become a useful encoding/representation that can be used for other
tasks.

**Inference**

Can be hard to calculate probabilities. Examples when we want to do
this: expected hidden value code for visible units, conditional
visible unit query (`p(y | x)`; `y` isn't hidden per se, because it is
observed in the training set), maximum likelihood training (because we
need `p(x)`).

Most models we use won't allow exact inference. But approximate
inference can be fast. Most common DL approximate inference technique
is variational inference.

**Deep Graphical Models**

They mention that the big distinction is that latent variables have no
explicitly intended purpose in Deep Models. We figure that the model
will learn that.

Deep models also have dense connectivity to latent variables. Thus
Loopy Belief Propagation, a typical approximation approach in sparse
networks, doesn't work well in these dense networks. So a big
difference from classic PGM is that we don't use LBP.

**Restricted Boltzmann Machine**

The quintessential deep undirected model. Single layer of hidden units
and of visible units. Only pairwise connections between hidden and
visible units. Dense connectivity. Designed to be easy to perform
Gibbs sampling on.

For the RBM, we have `E(v, h) = -vWh` (if I fold the biases into dummy
variables).

RBM has factorial conditional distributions `p(v | h)` and `p(h |
v)`. The first is `\prod_i p(v_i | h)` and the second is `\prod_i
p(h_i | v)`.

Likewise, `p(v_i = 1 | h)` is easy to compute. We take `\sigma(b_i +
W_{i, :} h)`.

This makes it very easy to do *block Gibbs sampling*. So sampling can
be fast!

It is easy to calculate the derivative of the energy function: if you
know both `v` and `h`. That means it is possible to do maximum
likelihood if everything were visible. But I assume we have an EM part
here, where we use the derivatives for optimization in the inner loop?

Not quite! Training uses the contrastive divergence concept. My
approach suggested won't work because having the derivative of the
energy function gives you the derivative of the unnormalized
probability of an example, but does *not* give you the derivative of
the normalized probability, since a change to the potentials affects
not only `p\tilde_\theta(x)` AND `Z(\theta)`.

## Ch17: Monte Carlo Methods

There are two reasons to sample. The first is sometimes we literally
want to produce samples: sample images, or fill in missing parts of an
image. The other is that we may use the samples to help approximate a
sum or integral. The more samples we draw, the better our estimate of
the sum/integral. A good example is if we want to compute the gradient
of the log partition function, which can be intractable.

**Importance Sampling**

There is a technique called *importance sampling* which can be
useful. Here we sample from a distribution `q(x)` and take the
empirical average of `p(x)/q(x) f(x)`. Of course, this will converge
to the right expectation of `f(x)` no matter the choice of `q`.

What is the best choice for `q`? Here we want the `q` that minimizes
the variance after `n` samples. The best choice is that `q*` (the best
`q`) choose `x` with probability proportional to `p(x) |f(x)|`. This
basically says: be the same as `p`, except be biased toward `x` that
have large magnitude. This is probably most easily understood if you
think of rare events that are massive (e.g., black swans).

Note that the best `q*` requires a normalizing constant `Z`. Typically
it is not practical to calculate `Z`. But we can often choose `q`
values that are better than just `p`.

**Biased Importance Sampling**

There is also *biased importance sampling*, which doesn't require
explicit normalization. Here you sample from `q\tilde`, and weight
`f(x)` as usual `p\tilde(x)/q\tilde(x)`. But then you "normalize" by
the sum of `p\tilde(x) / q\tilde(x)`, which approximates the
proper normalization constant.

This actually results in a biased estimate for finite `n`, but it is
*consistent*: it will converge to the right answer in the limit,
because then the normalization will approach the correct value.

**Sampling in High Dimensions Is Hard**

If `q` is a uniform distribution on a high-dimensional space, then it
is often that `q(x) >> p(x)|f(x)|`, which means you will sample
useless values that either don't actually occur very often, or are
very small in magnitude and don't contribute a lot for that reason. On
the other hand, for presumably fewer `x` values where `q(x) <<
p(x)|f(x)|`, you will have a situation where you are not sampling
important values that have big impact, but don't occur very often.

Unfortunately, when we have many dimensions the distribution can have
"high dynamic range," which makes the scenario described above
common. Still, importance sampling is used pretty often.

**MCMC**

The idea is that `p` may be hard to sample from, and that there is no
good `q` that will give you low-variance.

MCMC is best understood for discrete case first. Say you have a
distribution over `k` states. That's a vector in `R^k` that sums to
one. Now, consider a stochastic matrix `T`. Now, there's some proof
that a stochastic matrix always has a real, principal eigenvector with
eigenvalue equal to one. That means, that `T^k v` will approach this
eigenvector as `k \to \infinity`, no matter the initial choice of
`v`. This is the *stationary distribution* of the transition matrix
`T`. (So long as `v` is not orthogonal to the eigenvector, which is
probability zero under any reasonable initialization distribution).

The point is this: let's choose a transition matrix such that in the
limit it will converge to the right stationary distribution: the `p`
that is desired!

This shows the mixing, but if at each step you sample from the
calculated probability distribution, then in the limit your sample is
distributed as if by the principal eigenvector.

The same trick holds for continuous distributions.

After mixing (aka *burn in*), ever sample has probability `p(x)`, but
of course subsequent samples are not independent.

You can *thin*, by dropping samples until you feel like you have
wandered far enough awawy. Alternatively, you can have totally
uncorrelated samples if you use different chains. This could exploit
parallelism. But I presume mixing time is less than reasonable
decorrelation time, so overall CPU utilization would be higher.

Unfortunately we don't typically have very good tests for whether a
chain is mixed. Theoretically, if we had a discrete case, we could
examine the second eigenvector, and see if the eigenvalue is
small. However, if we have a number of variables, the number of states
is exponential in the number of variables, and the number of entries
in a stochastic matrix is that squared...

**Gibbs Sampling**

Nowhere have we said how we should construct the transition function
for our MCMC algorithm. We need to discuss that!

This says, pick a single value and update it based on its conditional
probability given the values of the other variables. This is typically
easy. This is *Gibbs sampling*.

*Block Gibbs sampling* is when you update many variables
simultaneously. You can do this when all the variables updated are
conditionally independent given their neighbors. This is for instance
*exactly* the case when dealing with the RBM.

There are other possible MCMC approaches like Metropolis-Hastings. I
forget how that works. But they say that Gibbs is much more widely
used in DL than MH.

**Sampling Between Modes**

The problem is that it can take a long time to wander from one area of
high probability to another if there are *very* improbable spaces in
between. This in particular happens when there are strong dependencies
between variables.

They show an example of a sharp, very skewed normal. It's hard to walk
from one side to the other, because the steps you can make are very
small.

All else equal, if you can sample highly dependent variables in a
block, then that is ideal. So, for instance, I expect blocked Gibbs
for RBM is probably way better than pure Gibbs.

All the same, they show a problem with MNIST, where it's really hard
to move from one class to another. The reason is that the `h` code is
still very highly dependent on the `v`, and vice versa.

The fundamental problem is: we want `x` to be highly dependent on `h`:
that means `h` is retaining the info for `x`. But the more we succeed
at this, the harder it will be to wander.

In particular, they note that MCMC is going to have a hard time
especially when the manifolds for different classes are
disconnected. Then it's going to be really hard to wander. But that is
really unfortunate because that's exactly the case we typically
expect!

**Tempering**

Here, we throw in a temperature parameter. That is, we scale the
energy by `\beta`: `p(x) ~ \exp(-\beta E(x))`. The higher `\beta` is,
the "colder" the environment is, and the probability distributino
becomes more sharply peaked. For small (positive) `\beta`, the higher
the temperature, and the more uniform.

By changing the temperature, we might be able to help ourselves move
between modes. However, they mention that this approach has not yet
shown a lot of fruit, because we have to be very careful how we change
the temperature.

**Depth**

There is a note, which I think they just add for interest, that deeper
representations seem to be more unimodal than shallow
representations. Experimentally, I think they trained a deep
autoencoder representation, and then used this for an RBM and the RBM
mixed better.

However, they note that it isn't clear how to use this insight
practically. So they seem to include it just to help you build
intuition.

## Ch18: Confronting the Partition Function

We talked about how to sample. But now we need to talk about how to
optimize a model with a partition function. Returning to the RBM
example: we praticularly want to know the gradient of `Z` as a
function of `\theta`. We need that to do gradient descent: we can
easily calculate the derivative of `E(v, h)`, but that doesn't tell us
how a change to `\theta` affects `Z`. If `E(v, h)` goes down a lot,
but `Z` goes down even more, then that means that `p(v, h)` is *less*
probable.

Everything makes sense in the log probability space. We have:

    \log p(v, h; \theta) = \log (p\tilde(v, h; \theta) / Z(\theta))
    = \log p\tilde(v, h; \theta) - \log Z(\theta)

This decomposition consists of a *positive* "phase" (the term where
you increase the unnormalized probability of `v, h`) and a *negative*
"phase" (the term where you try to decrease the partition function
`Z`. Effectively: decrease the unnormalized probability of all other
configurations. When these forces are equal, you've got MLE.

Because the partition function sums out/integrates over all variables,
it is typically the more difficult quantity to calculate. The positive
term is the easy part.

Now, we can show that the gradient of `\log Z(\theta)` is equal to the
expectation of `\grad_\theta \log p\tilde(x)` for `x ~ p(x)` according
to the model. That's actually sort of interesting: the gradient of
`Z(\theta)` would be the *sum* of the gradients, not the
expectation. But it is also logical: you are comparing what you are
doing on the positive example to the change you make at a "typical"
example.

This is basically the idea: we want to (1) push down on the energy at
training set datapoints, and (2) pull up on the energy everywhere, in
particular proportional to the probability that the model assigns to
those datapoints.

This is basically saying: model, correct yourself by making your
incorrect beliefs less likely, and your correct beliefs more likely.

**Naive MCMC**

Because an expectation is involved, this is a classic time to do an
MCMC thing!

What you do is this. Take a mini-batch. Calculuate the positive
gradient.

Next, generate a sample by mixing an MCMC chain. Find the gradient
with respect to `\theta` of the *hallucinations*. Now subtract the
positive and negative gradients and use it to update.

The problem is that you're burning in an MCMC chain in the inner loop,
which is very slow. Possibly ~100 MCMC steps will be needed for a
small image patch.

**Contrastive Divergence**

The idea is that you can avoid long burn-in time if you start the
chain out in a distribution that is approximately the target
distribution.

The model `p(x)` is supposed to converge to the true, target
distribution. So you can start your MCMC by sampling a random example
from the training dataset. This is sampling from the target
distribution, right?

Now, you need a sample from *your* distribution. So you still have to
do MCMC, using maybe 1-20 Gibbs sampling steps. By using a reasonable
starting point, rather than randomly sampling an initial point,
burn in/mixing requires far fewer steps.

It is typicaly to just use your mini-batch and do a number of Gibbs
sampling steps.

They note that in the beginning, when your model distribution is very
far from the true distribution, you probably won't properly mix. While
you may push down on the probability in the wrong places, you will
still pull up in the right places. As you get closer to the true
distribution, the initialization from the dataset should be more of a
good starting point and the chain will mix better.

**RBM Training**

Indeed, this is the traditional way to do RBM training. You take an
example `v`, and sample `h`. You use `v, h` to calculate your positive
gradient. It is worth noting: this isn't exactly `\grad_\theta
p\tilde(v)`, but it is an unbiased estimate. To sum out over all
`2^|H|` hidden states is presumably impractical.

Then you resample `v'`, and then `h'` too. Use `v', h` for
your negative gradient calculation.

**CD Problems**

The first problem is *spurious modes*. These are areas of high
probability in the model which are of low probability in the
data. These tend not to be visited/actively suppressed if you use CD,
since you're initializing with data from the training distribution.

CD doesn't help much with deeper models. That's because with deeper
models it is hard to obtain samples of the hidden units. I assume this
is because in an RBM you don't need to initialize any hidden units;
you can select just based on the `v` from the distribution. If you
have two layers, you need to initialize `h_2` to sample `h_1`, or vice
versa to sample `h_2`. If this initialization is random, then it won't
be like the underlying distribution.

**Persistent Contrastive Divergence**

This technique is also called *stochastic maximum likelihood*. The
idea is very simple. You initialize the model with samples that were
drawn in the previous iteration. The idea is that if the model is
changing slowly, then the last samples are still fairly likely under
the model, and are a good place to start.

This can be a good strategy for deep Boltzmann machines. They suggest
on a small image patch you might need 5-20 steps of the chain to mix.

However, you must not change the model too quickly, or you must run
the chain longer between samples. And it can be hard to know that
you're doing this correctly, just like it's always hard to evaluate
mixing.

**Pseudolikelihood**

Principle is that to calculate `p(A = a | B = b)`, (assuming the set
of variables `A` and the set `B` union to the whole space)you need
only calculate `p\tilde(A = a, B = b)` and divide by `\sum p\tilde(A =
a_i, B = b)`. Basically: you don't need the entire partition function.

If there an additional set of variables `C`, you'll have to sum that
out.

So it's potentially quite simple to calculate `p(A = a | B = b)` if
`A` consists of a single variable and `B` consists of all the rest. (I
mean, this is basically why we can do Gibbs sampling, right?).

Now, the log likelihood `\log p(X = x)` is equal to `\log p(X_1 =
x_1) + \log p(X_2 = x_2 | X_1 = x_1) ...`. The difficult here is that
we have to marginalize over some large sets of variables. Basically,
that first term is the worst to calculate!

So the *pseudolikelihood* approach is this: just sum out `\log p(X_i =
x_i | X_{-i} = x_{-i})`. Clearly this is *far* simpler to
calculate. This may seem like a ridiculous hack, but in the limit of
data, it is asymptotically consistent and maximizing this should give
you the MLE. Of course, you may not have an infinite amount of data
;-).

There is a concept called *generalized pseudolikelihood*. This is a
simple extension: instead of always doing a conditional probability
for `X_i` using everything else as given, you partition the `X` into
sets `S_1,\ldots,S_k`. Then you condition `p\tilde(X_{S_i} = x_{S_i} |
X_{-S_i} = x_{-S_i})`.

This can be very useful if you know some sets of variables have strong
interactions; the stronger an interaction, the more standard
pseudolikelihood will break down. One example when you can know this
is presumably for images.

It is important to note: it can be hard or impossible to use
pseudolikelihood when you can only approximate `p\tilde`. The reason
is that you can't always use an approximation as a denominator, and
that is what you are doing with pseudolikelihood. A particular case is
when you are using a lower-bound approximation to `p\tilde`. We'll see
that pseudolikelihood doesn't play nice with variational inference,
which uses lower bounds.

In particular: variational inference is a primary way to train deep
Boltzmann machines, so pseudolikelihood is hard or impossible to make
work in that context.

**Pseudolikelihood and RBM Training**

The original Hyvarinen paper talks about pseudolikelihood on *fully
visible* Boltzmann networks. They mention that extensions to hidden
variables is an "important subject for the future."

In the Tieleman PCD paper, he compares with pseudolikelihood *for
fully visible Boltzmann networks*.

It wasn't immediately obvious how to do pseudolikelihood for
RBMs. Now, we know that we want to calculate `p(V_i = v_i | V_{-i} =
v_{-i})`. We know that means calculating `\sum_h p\tilde(V_i = 1,
V_{-i} = v_{-i}, H = h)`, and the same for `V_i = 0`.

The problem is the summation involves summing out `2^|H|` terms. It
seems like the conditional independence of `H` on `V` should help. It
does.

We know that:

    p\tilde(v, h) = exp(v^T b_v + (v^T W)h + h^T b_h)
        = exp(v^T _bv) \prod_i exp[ ((v^T W)h + h^T b_h)_i ]

Now, to calculate `p\tilde(v)`, we must marginalize all `h`
vectors:

    p\tilde(v) = exp(v^T b_v) \sum_h \prod_i exp[ ((v^T W)h + h^T b_h)_i ]

Note that we can split the vectors in half: those where `h_1 = 0`, and
those where `h_1 = 1`. Thus we can write:

    p\tilde(v) = exp(v^T b_v)
                 [exp(0) + exp[ ((v^T W) + b_h)_1 ]]
                 \sum_{h_{2:K}}
                 \prod_{i = 2}^K
                    exp[ ((v^T W)h + h^T b_h)_i ]

And of course we can just continue for all features. This becomes:

    p\tilde(v) = exp(v^T b_v)
                 \prod_i
                 [exp(0) + exp[ ((v^T W) + b_h)_i ]]

This shows that the `p\tilde(v)` can be calculated very rapidly and
has a simple analytic form. Which means we can take its derivative
very simply.

Now, we want to *maximize* `p\tilde(v)`, but we can equivalently
*minimize* `-log p\tilde(v)`. This quantity is called the *free
energy* of `v`:

    -\log p\tilde(v) = -(v^T b_v)
                       - \sum_i \log(
                           [exp(0) + exp[ ((v^T W) + b_h)_i ]]
                       )

https://stats.stackexchange.com/questions/114844/how-to-compute-the-free-energy-of-a-rbm-given-its-energy

Cho in his master's thesis argues that pseudolikelihood doesn't seem
to be a good way to train BMs or RBMs. It doesn't appear to be a good
approximator of the likelihood.

https://pdfs.semanticscholar.org/6c20/07f69d44a4b302bded40eaa28ce5fa543de2.pdf

Pseudolikelihood can be interpreted as having a "negative" phase in
the sense of trying to suppress examples that are different at one
coordinate.

**RBM: Use Exact Gradient Of E(v)!**

I was very confused about CD. Once I saw that you could directly
calculate `F(v)`, I started to ask why we don't minimize:

    -log p(v) ~ F(v_pos) - F(v_neg)

I assumed we were trying to minimize

    -log p(v_pos, sampled_h_pos) - -log p(v_neg, sampled_h_neg)

However, that is *not* what is suggested, and you can verify this in
Murphy's Machine Learning book, specifically algorithm 27.3.

It explicitly shows: you do MCMC to sample a new `v_neg`, then you do
gradient descent on the log probability as above. In doing MCMC, you
always use binarized hidden vectors.

CD is explicit about how to calculate the *gradient*. Let's do it
ourselves:

    -\log p\tilde(v) = -(v^T b_v)
                       - \sum_i \log(
                           [exp(0) + exp[ ((v^T W) + b_h)_i ]]
                       )
    =>
    \grad_{W_{i, j}} -\log p\tilde(v)
    =
    \grad_{W_{i, j}}
    - \log( 1 + exp[ ((v^T W) + b_h)_j ] )
    )
    =
    - 1 / (
        1 + exp[ ((v^T W) + b_h)_j ]
    )
    exp[ ((v^T W) + b_h)_j ]
    v_i
    =
    -v_i \sigma( [v^T W + b_h]_j )
    =
    -v_i E[h_j | v]

Look at that! The gradient is exactly the outer product of `v` *and*
the probability vector `h`. *That is what Hinton is suggesting*. He's
giving us the *update* rule in terms of the probability vector `h`;
he's *not* saying it's part of the objective, because we already saw
all `h` values were marginalized out.

Huzzah! Thank god this nightmare is over!

**Score Matching/Ratio Matching**

Score matching is another approach when you have a closed form
unnormalized probability distribution. If you have that (and if you
have continuous observed variables), then you can use score matching.

Your goal is this: minimize the sum across all datapoints `x` of the
L2 norm of the difference in *score* of the data distribution and the
model distribution at `x`. The score is:

    \grad_x p(x; \theta)

You don't need Z to calculate that. But you need the *data
distribution*. If you had that, you wouldn't be doing any of this!

But Hyvarinen showed that you can just minimize the sum across all
datapoints `x` of:

    \sum_i (log p(x))''_i + 1/2 [(log p(x))'_i]^2

where `i` ranges over all dimensions.

You can't use score matching when you can't calculate a derivative,
which means pretty much any approximation of `p\tilde` won't work with
score matching, since approximating `p\tilde` has nothing to do with
matching the derivative.

So it is very hard to see how to use score matching for deep
networks. Moreover, because the nodes in the first hidden layer are
probably discrete, you can't apply score matching to pretrain each
layer either.

All this is too bad, because score matching doesn't involve MCMC,
which is great.

There is another approach called *ratio matching*. Here you want to
minimize a loss `\sum (1 / (1 + p\tilde(x)/p\tilde(f(x))))^2`. `f`
corrupts `x` by flipping a bit at a position. It avoids the partition
function in the same way as pseudolikelihood.

It sounds a lot like maximizing pseudolikelihood: you push down on all
the corruptions of `x` by one bit.

**Noise Contrastive Estimation**

This approach basically tries to learn the normalization
constant. How?

We introduce a noise distribution that generates samples `x`. The
closer the noise to the true distribution the better, but maybe just
uniform or gaussian noise will work.

We then create a dataset of `(x, y)`, where `y = 1` for true samples
`x`, and `y = 0` for fake samples.

We now reframe our problem: we now have a maximum likelihood problem
for classification of examples as true or false. Now, we can do some
analysis ourselves:

    p(y = 1 | x) = p_model(x) / (p_model(x) + p_noise(x))
        = 1 / (1 + p_noise(x)/p_model(x))
        = \sigma(-log p_noise(x) / p_model(x))
        = \sigma(log p_model(x) - log p_noise(x))

Now, the only thing we can tune is `p_model(x)`. Basically: we want to
find the best `p_model(x)` to optimize the loss.

What is `p_model(x)`? This is normally hard to say because we don't
know the normalizing constant. But here we're going to learn an extra
parameter: `c`, which is our estimate of `Z`. We'll compute the
probability in the normal way:

    log p_model(x) = log p\tilde(x) - log c

Asymptotically we should learn a `c` that approaches `Z` and properly
normalizes. But while we're learning, it's fine that `c` does not
quite normalize. This is tolerated by the definition of the problem.

Notice that we don't want to just make `c` very small. If we did that,
we would also think that *noise* was more probably from the model.

**Negative Sampling**

(My note)

NCE is very similar to *negative sampling*. I used negative sampling
when learning word embeddings. Remember how this works:

    (1) a layer of context, which is like a 50k one hot.
    (2) a matrix of embeddings
    (3) a layer of predictions, which is ~50k words again.

Here's the idea, if the embedding has like 200 units, and there are
50k words, then we're talking like 1MM connections to calculate the
probability distribution.

So the idea of negative sampling is to just pick a subset of *negative
samples*, and train the softmax against these. For instance, if you
picked 100 negative words, that would mean you train (101 * 200)
weights this iteration. This is much more efficient.

The training objective is now a little monkeyed. It turns out that
negative sampling is not asymptotically consistent on identifying the
probability distribution. Thus it is only good for learning feature
embeddings.

Dyer has some analysis: https://arxiv.org/pdf/1410.8251.pdf

**Negative Sampling vs NCE**

I'm going to imagine that NCE basically does just like negative
sampling, except it doesn't look at 100 or so negative activations to
do a softmax. Instead, I assume it learns a parameter `c` that tries
to learn the unnormalized sum of the softmax layer. This ought to be
consistent.

In fact, you can just tell NCE to assume `Z = 1`, and then instead of
softmaxing, I guess you just do a sigmoid at each point?

The Dyer paper notes: if you are trying to develop a *generative*
model of language, you should be doing negative sampling.

The word2vec paper (https://arxiv.org/pdf/1310.4546.pdf) talks about
NCE vs Negative Sampling. It basically says that NCE is overkill for
their purposes.

**When To Use NCE**

NCE is a classic choice for a problem like word embeddings. In the
word prediction task, there is really only one target variable being
predicted, but it takes on many many values.

NCE breaks down when you have many variables, even if they only have a
few values, because it's too easy. Remeber: NCE is like a stupid
GAN. In particular, say you're trying to learn to recognize faces. The
problem is that if you're training against randomly generated face
noise, then as soon as you notice eyes or mouthes, you have an
ironclad way to reject the noise samples.

**Self-Contrastive Estimation**

Here, your noise sample is *the same model*. So you want to try train
a better and better version of the model, which each time understands
the bullshit of the previous version.

Wait: isn't this like literally the same as contrastive divergence?
SCE seems like an idea that no one cares about besides Goodfellow, so
it hasn't been studied very much. There are like only two or three
papers that mention it at all.

**Partition Function Estimation: Importance Sampling**

Estimating the partition function can be important so that we can
evaluate model performance.

To compare two models, we would look at the difference in log
likelihood of the data. This breaks down into the log of the ratio of
the unnormalized probabilities in both models, minus the log of the
partition function ratio. If we can estimate this ratio, we are good.

Let's use that idea to approximate `Z_1` for a model of
interest. We'll use a model `p_0` designed to be easy to (1) calculate
`p_1\tilde(x)` and (2) `Z_1` easy to calculate.

If we do that, then we can use this to do a Monte Carlo estimation of
`Z_1` like so:

    Z_1 = \int_x p_1\tilde(x)
    Z_1 = \int_x p_0(x)/p_0(x) p_1\tilde(x)
        = \int_x p_0(x) Z p_1\tilde(x) / p_0\tilde(x)
        = Z \int_x p_0(x) p_1\tilde(x) / p_0\tilde(x)

Obviously we can do a Monte Carlo approximation by:

    Z_1 = Z_0/K \sum p_1\tilde(x) / p_0\tilde(x)

This is basically exactly importance sampling. Basically it treats the
target distribution for `x` as uniform (integrate over entire space),
and the proposal distribution as `p_0(x)`. It multiplies the sum of
`p_1\tilde(x)` by `1/p_0(x)`.

This will work for *any* proposal distribution, but it will only be
efficient if there is a `p_0` similar to `p_1` which is tractable. But
this is a challenging burden: you want to do this Monte Carlo thing to
calculate `p_1`, which is a complicated distribution, but now you're
asking for a `p_0` that is simple and close to `p_1`??

**Annealed Importance Sampling**

Here's the idea. You start with a simple distribution to sample from,
and you then use this to generate samples from a series of
distributions, each one closer to the true one than the last.

I don't want to get too much into this. The idea seems not super
profound, but clever. The book says this is the most popular technique
for estimating the partition function of an RBM. But the book also
says this may be for no good reason, and that other techniques might
work better.

*Bridge sampling* is similar. It basically introduces a `p*`
distribution, and you try to estimate `Z*` using `p_0` AND `p_1`. Like
so:

    Z_1/Z_0 ~ (\Sum p\tilde*(x) / p\tilde_0(x))
              / (\Sum p\tilde*(x) / p\tilde_1(x))

As we know, the numerator and denominator are importance sampling
estimates of `Z*/Z_0` and `Z*/Z_1`. This approach can work well if
there is a good intermediate distribution between `Z_0` and `Z_1`. In
fact, you can iteratively choose better and better `Z*`.

## Ch19: Approximate Inference

You want to maximize the likelihood of your data, `p(v)`. But as we
know this can be intractable to calculate.

**Variational Inference In Bits!**

Here is an equation:

    nlog p(v) = E_{h ~ q}[nlog p(v, h)] - H(q) - D_KL(q(.) | p(.|v))

Where does this come from? Here's the idea. If we use `p(v, h)` to
encode samples from `q`, this is stupid: `p(v, h)` doesn't even
integrate to 1 over all `h`. The problem is that there is a part of
the code for `(v, h)` that really just represents `v`: this has `nlog
p(v)` bits.

Now, if `q = p(.|v)`, then we would have:

    nlog p(v) = E_{h ~ p(.|v)}[nlog p(v, h)] - H(p(.|v))

This is saying: if you know the expected code length of this stupid
code, then subtract out the entropy of `p(.|v)`, which is the optimal
code length, and that gives you the part you were wasting by coding
`nlog p(v)`.

However, if we use some `q \ne p(.|v)`, then

    E_{h ~ q}[nlog p(v, h)] - H(q)

overestimates the number of bits needed to store `p(v)`. The reason is
that this is that `p(.)` doesn't give an optimal encoding for
`q(.)`. The excess comes from `D_KL(q(.) | p(.|v))`.

As you know, we call `nlog p(v)` the *free energy* of `v`. Likewise,
we call:

    E_{h ~ q}[nlog p(v, h)] - H(q)

the *variational free energy* of `v` (for the distribution `q`). It is
by necessity an *upper bound* on `nlog p(v)`. For better choices of
`q`, we will have `H(p(.|v))` be smaller, and the upper bound will be
tighter. When `q(.) = p(.|v)`, we will have the variational free
energy equal to the true free energy.

**Evidence Lower Bound**

*Evidence lower bound* is for people who like maximizing log
likelihood more than minimizing encoding length. Those people are bad
and should feel bad. Just kidding! Maybe...

Anyway, the ELBO is just negative variational free energy. It is a
lower bound of course.
