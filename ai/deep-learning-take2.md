These notes are supposed to go through Part III a second time for
better understanding.

## Ch13: Linear Factor Models

**PCA**

All these models are linear in the latent factors. They account for
any inability to fit a linear model with a noise.

Factor Analysis assumes a prior on the latent factors that is unit
normal. Noise is also normal, and independent: covariance matrix is
diagonal.

Probabilistic PCA just further assumes that the normal noise added has
constant variance. We recover PCA if the added normal noise is shrunk
to zero. It feels like whatever the additive noise, the MLE of the
linear transformation of `h` to `x` should always be the same, by
analogy to how with linear least squares regression, it doesn't matter
the variance of the additive noise. But I may not be entirely correct
about that. I am clearly a little confused on this point.

**Independent Component Analysis**

Used normally for separation of output into inputs. For instance, if
we have multiple speakers, or if we have an EEG and need to sort out
electrical signals from brain vs heart.

We again have a prior distribution on `h`, and then `x` is
deterministically generated by a linear transformation. Theoretically
we could learn via MLE, but a typical training goal is instead to try
to make the decomposed `h = W\inv x` as *independent* as possible.

It is required that the prior be non-Gaussian. If not, then the best
`W` is not identifiable, and you can mix the `h` around however you
want.

Many versions of ICA are "agnostic" of the choice of `p(h)`. Instead,
you try to learn a `W` such that the sample `W\inv x` is as
non-Gaussian as possible: for instance, by maximizing *kurtosis*. I
guess the idea of finding non-Gaussian signals makes sense to me:
Gaussian typically means mixture of factors.

They then talk about some nonlinear extensions. They also talk about a
version that has some kind of mixture modeling, where certain factors
are allowed to have statistical dependence. These include having
explicit groups which can have dependencies between factors in the
group (independent subspace analysis), or *topographic ICA*, which is
interesting. It assigns each factor a position in a space, and that
the closer two factors are in the space, the more dependence they are
allowed to have.

**Slow Feature Analysis**

Basic idea is to regularize a model of data that has a time dimension
to prefer features that change slowly over time.

Incidentally, you must require that features have variance equal to
one, else you can just "shrink" features to be basically zero. We also
need features `i` and `j` to have zero linear correlation (are
orthogonal) so that we learn different features rather than the same
one.

To learn non-linear features, we do quadratic basis expansion (add all
cross-terms). You can even do a deep version by repeating this process
iteratively.

SFA doesn't seem to give state-of-the-art performance. The idea is
that features don't necessarily have predictive value. Maybe we should
be trying to learn features that accurately predict the next
frame. Likewise, there may be a bias against predictive features that
change rapidly.

**Sparse Coding**

This is a slightly weird one. Again, it's a linear model with gaussian
noise added. The prior on `h` should be sparsifying, like the Laplace
distribution. Typically we assume a factorized distribution (`h_i` are
all independent).

Sparse coding with MLE is apparently intractable. This is presumably
similar to any situation with hidden latent variables. However, we can
use EM. We first pick the best codes `h` for the examples `x`. Then we
modify the weight matrix to maximize the probability of the observed
`x`s for the calculated `h` codes.

We don't typically try to learn a *parameteric encoder* that
calculates/estimates the best `h` for a given `x`. Instead, we do an
optimization procedure to step by step build a better `h`. This
optimization procedure doesn't have any parameters. The optimization
is the weighted sum of L1 error for `h`, and L2 reconstruction error
for `x` using the current `W` applied to `h`.

When going to use the model, this is in a sense an ideal encoding
strategy. For parameteric encoders, we have to worry about how well
they would encode an unseen `x` that may be quite different from the
observed `x` values in the training dataset. Instead, with our
non-parametric encoder we need only worry about the *decoder*
generalization.

However, it is slow because you have to run an optimization
algorithm. Also, the iterative algorithm means you can't backpropagate
through the encoder.

Because you assume a factorial distribution on `h`, you often generate
poor samples. This is similar to most other linear factor models. This
happens even when the you can reconstruct data well. That will drive
us toward considering deeper models that don't have a factorial
distribution on their deepest code layer.

## Ch14: Autoencoders

Consists of an encoder and a decoder. But I guess the idea is that
these could be deeper, or have a nonlinear mapping from `x` to `h` and
`h` to `x`.

There are deterministic encoders/decoders, and also stochastic ones. A
stochastic encoder would compute a set of parameters `\Theta = f(x)`
which gives a distribution over codes, while a stochastic decoder
would compute a set of parameters `\Theta = g(h)` to give a
distribution over `x`. Basically the two networks are learning
conditional inference. In sec14.4 they mention that to keep things
simpler, it is typical that the form of distribution on `x` is assumed
factorial. But sometimes you have mixture models at the end.

Autoencoders are typically used for dimensionality reduction and
feature learning.

**Undercomplete**

First strategy to consider is *undercomplete* autoencoders. They must
learn a latent representation that contains less bits than the input,
but can still reconstruct the input well. These should be the most
salient features of the input. If the decoder is constrained to be
linear and the loss is squared error, then the encoder is just
learning the PCA encoding.

By using deep models, we can generalize from PCA by allowing
non-linear encoding/decoding. That is, projection onto a non-linear
manifold.

However, if we have two much capacity, we may not encode useful
information in the latent variables. For instance, let's consider an
encoder where `h` is interpreted as an integer in binary
representation. If `h = 123`, then that means example number 123,
which the decoder has memorized. This would be a stupid
representation.

**Regularized**

Consider *overcomplete* codes; even a linear encoder can just copy all
the input information to the code trivially. Rather than limiting
encoder/decoder capacity, we will impose regularization to try to
ensure the code is useful.

**Sparse Autoencoders**

Sparse autoencoders imposes an L1 penalty on the codes. Now, unlike L2
regularization on weights, which can be interpreted as implying MAP
estimation with a gaussian prior on weights, this is a regularization
on *code*.

However, we can still interpret this as a prior *on codes*. In this
case, we are learning an MLE model, where there is a prior on codes,
*and* we are using just the single most probable estimate of `h` to
estimate the likelihood computed for `x`.

You can thus see the autoencoder as approximate training for a
generative model.

**Denoising Autoencoders**

We corrupt the input and then try to reconstruct to the original to
repair the damage. I believe this is basically saying that, as you
leave the manifold of high probability, the change in reconstruction
should be low.

They talk about *score matching*. This says that, equivalent to
learning `p(x)` is to learn `\grad_x log p(x)`. The relation to DAE is
this. The DAE learns a model where the score it assigns to `g(f(x))`
matches the score of the data distribution. That is, in directions
perpindicular to the manifold of high probability (directions in which
the score is zero), there should be no change in `g(f(x))`.

They talk a lot about DAEs in relation to RBMs, but I don't understand
this because I don't know RBMs well enough yet probably.

**Manifold Learning**

Early attempts were to, at each data point, impute a tangent plan
which spans the directions of variation associated with the differene
between an example and its nearest neighbors. This formed an
approximation of the manifold. The problem with this approach was that
it works for interpolation but not extrapolation.

**Contractive Autoencoders**

CAEs impose a regularization constrain on the encoder: the derivative
of `h = f(x)` with respect to `x` should be small. We usually compute
this via the Frobenius norm. It appears that when we are trying to do
representation learning, CAE tends to work better. Presumably that's
because you don't get any credit for resisting changes to `h` in the
decoder. That means maybe you learn a better representation `h`, since
the decoder may be sensitive to changes in `h`. (The paper by Rifai
seems to confirm my hypothesis).

The name comes from the idea that the encoder is trying to "contract"
the space to codes generated by the `x` values. Those directions with
high derivative tend to have semantic meaning.

A problem is that it can be hard to calculate the Jacobian in deeper
networks. Why is this particularly difficult?

One subtlety: if you let the decoder expand the space arbitrarily,
then the encoder can just shrink the space, learning nothing. So you
have to put some kind of constraint on the decoder's ability to
expand. The approach used by Rifai (maybe the one most interested in
CAEs) is to use transposed weights in the decoder. (In the paper, they
say that using the transpose weight matrix in the decoder is common,
but I don't know that I understand why. I mean, for a single linear
transformation maybe that makes sense? But even in a full-rank case,
the transpose is not the inverse...).

I've seen comments which suggest if `W` is the (transposed)
eigenvector matrix, then assuming no nonlinearity, this is just
PCA. And so optimization will learn `W`. But according to
(https://groups.google.com/forum/#!topic/theano-users/QilEmkFvDoE)
lots of people also use untied.

It looks like outside CAE, maybe people use tied weights because they
don't want you to use really small weights in the encoder, so that you
can exploit the linear activation function, and then use really large
weights in the decoder. But I'm getting into weeds maybe...

**Predictive Sparse Decomposition**

A mashup of sparse coding with nonparameteric encoding and parameteric
encoders. Your new loss function sums:

(1) L2 reconstruction loss.
(2) L1 loss of best `h` encoding found by iterative optimization.
(3) L2 loss of parameteric encoder `f(x)` to `h`.

You do EM. You find the best `h`, then you optimize `f` and `g`, then
do it again. One advantage is that `f(x)` is a very natural
initialization for `h` in your optimization algorithm.

When you go to do encoding online, it's now much cheaper thanks to
your parameteric encoder.

This is just a little diferent from learning a sparse coding model and
then training an encoder. The reason is because with PSD we can still
modify `g` to be compatible with a sparse coding that `f` does a good
job of predicting.

An advantage to PSD is that now you have a model which is fully
differentiable. That means it is a good choice for initialization of a
deep network.

**Semantic Hashing**

They mention that you can do semantic hashing if you can force the
autoencoder model to output ones and zeros. You could just round
logistic outputs, but they recommend explicitly training the model for
this purpose by slowly increasing an added gaussian noise to the
activations. That will make your encoder want to saturate the
sigmoids.

Now, when deploying the model for semantic hashing, you don't add the
noise, and just round to zero or one.

## Ch15: Representation Learning

The tension is between retaining information about the data and other
desirable properties, such as independence of variables.

Deep networks are often initialized with a stack of autoencoders or
RBMs. This is unsupervised greedy layer-wise pretraining. They claim
this isn't needed any more for densely connected networks, but not
why. This
(https://www.reddit.com/r/MachineLearning/comments/22u1yt/is_deep_learning_basically_just_neural_networks/cgqgy9w/)
claims that ReLU and dropout obsolesced pre-training, since (1) you
can propagate signal better and (2) dropout provides good
regularization. It is noticed that greedy pre-training doesn't seem to
be very helpful. But they do note that when you have large unlabeled
sets and small labeled sets, unsupervised pre-training can be helpful
still.

Aside: that thread has significant debate on whether sigmoids or ReLUs
are better. There is a claim that maybe sigmoids are better for
regression? Others say that ReLU isn't typically used for
autoencoders, and also that the benefit is really for deeper nets. I
don't feel like there is a clear explanation at the moment for me why
one would be better at some tasks and the other better at other
tasks...

For supervised tasks, sometimes this doesn't decrease training error,
but *does* reduce test error. That makes sense, because the network is
maybe biased toward weights that pass forward more information about
examples that aren't supervised-trained on. In this sense I think
Goodfellow considers it regularization.

There are many ways to do semi-supervised learning. One is virtual
adversarial, another is to try to minimize cost of a combined
unsupervised and supervised network. But the focus here is
pretraining.

They say there are two parts to the pretraining idea. First, that
pretraining has a "regularizing" effect (per above) and (to a lesser
extent), can improve optimization on the training set. The other part
of the question is: why does learning about the input distribution
help you solve the classification/regression problem? They admit this
isn't totally well understood.

It's not clear how the regularizing comes about. At first, people
thought it was because you were biased to certain local minima. But
experience shows that local minima aren't really a problem. My theory
would be that the starting weights bias you toward settings of the
weights that look equivalent, but work better for the test set. It's
not that this area is inaccessible, but that it has no attracting
power.

They mention that the unsupervised hopefully forces the data into a
representation that is useful to the supervised task. For instance,
hopefully the unsupervised codes linearly seperate the examples. But
it isn't well understood when this will happen. Therefore, the most
popular approach (I think), is to do the unsupervised and supervised
training simultaneously. I expect this means the supervised task will
tend to push the unsupervised task toward learning compatible codes.

Of course, the worse the original representation, the more useful is
unsupervised training. For this reason it is super-useful for words,
but less useful for images, since images do actually have more
semantic similarity embedded in L2 distance.

They suggest that the more complicated the supervised function, the
more attractive it is to regularize with unsupervised learning, versus
other weight decay type methods, which bias you toward simpler
functions which isn't really what you want.

Erhan explored unsupervised pre-training a fair bit. They found that
NNs always trained to the same region when pre-trained, which meant
there was less variance in outcome. That implies that there was less
overfitting. But why? There is a hypothesis that unsupervised training
encourages finding/use of the true causal factors.

They mention another disadvantage of two phases of
pretraining/training. You now have an outer loop to test pretraining
parameters, making everything slower. You also can't really choose how
you want to balance the effect of unsupervised loss versus supervised
loss: you either pre-train or not. So it seems even better to do both
simultaneously.

Experimentally, straight supervised training with dropout acheives
better performance on large and medium sized datasets. But then on
small datasets Bayesian approaches work better!

**Transfer Learning**

They then talk about transfer learning more generally. They mention
that a lot of times tasks share low-level features. Sometimes they
share *high* level features. For instance, speech-to-text might want
to keep the high layers, but swap out the low layers to be tuned for
different speakers.

They mention that empirically in transfer learing competitions (learn
on example set A features that are used for linear classification on
example set B), deeper representations seem to do a lot better with
far fewer examples in A.

They mention one-shot and zero-shot learning as extreme transfer
learning examples.

**What Makes A Representation Good?**

IT can be that if we learn the factors that cause `x`, then the `y` we
want to predict from `x` may itself be among those causal factors.

We normally try to impose sparsity or independence on the factors `h`;
that isn't the same as finding the causes. Why is that helpful? They
posit (without real proof) that once you identify causes, you can
generally separate these into independent factors. I don't know that I
totally believe that.

In general: if `y` is closely associated with the causal factors of
`x`, then representation learning ought to be helpful.

One challenge is that `y` may be associated with a cause of `x` that
might not otherwise seem to be among the most important causes. For
instance, imagine a task where you are asked: is there a house in the
background of the photo? Then if you use the L2 error for
reconstruction, this causal factor doesn't really impact the overall
image very much. In that case, it's not entirely clear if unsupervised
learning is going to be very helpful.

They note one solution idea: GANs. This allows us to learn what
details are salient and should be explained.

One advantage to learning causal factors is that if you learn `y | h`
(`h` is the causes of `x`), then this is robus to changes in the
distribution on `h`. This may be particularly important for transfer
learning, because when moving domains, the distribution on the
underlying causes of variation may change, but maybe the mechanics
stay the same. This is more likely to break if we learned things that
are *associated* with the causes.

They note that distributed representation is clearly important for
representation learning. This is what allows us to learn things about
cats and realize this can transfer to dogs. They give a geometric
intuition: that this lets us modify several different regions of space
by changing one weight.

I feel like this section is very hand-wavy and not saying much...

**Depth**

They claim that those factors that can be independent of each other
must be at a high level, because they interact at lower levels. Like
"roundness of face" and "big mouth" are pretty independent, but they
probably play out with complicated interactions at a lower level. That
suggests a need for *depth*.

**Asumptions That Help Us Find Underlying Causes**

If we're going to find true underlying causes, we need to have
assumptions that will bias us toward learning those causes. Otherwise,
we may learn features that are merely associated with the pheonomenon.

The most interesting of this laundry list (to me) is sparsity and
independence of high-level factors. The idea of sparsity is that most
features are not needed for most examples. For instance, a feature
about size of elephant trunk is not relevant to non-elephant
images. Likewise, independence of features. They also mention temporal
coherence, which relates to slow-feature analysis.

## Ch16: Graphical Models

Without structure, too many parameters. Sampling, density estimation,
conditional queries, et cetera are too hard. And statistical
efficiency is too low.

In a Bayes net, the number of parameters is potentially exponentional
in the number of parents (assuming binary variables). For Markov
network, it is exponentional in clique size.

Typical that we use an *energy function* where `p(x) =
exp(-E(x))`. This is a Boltzmann distribution; machines with this kind
of distribution can be called Boltzmann machines. Originally BMs had
only binary variables, but that isn't true anymore. BMs typically have
latent variables, otherwise we call these just Markov fields or even
just log-linear models.

Indeed, with this format, then you can just talk about each clique
setting having a certain energy, which is the log of its *clique
potential*.

By association with physics, the unnormalized marginal log probability
of `x` (that is, `log p\tilde(x)`) is sometimes called the *free
energy*. We can calculate this by summing out over all `h` `E(x, h)`.

It's not entirely clear to me when to prefer Bayes nets versus Markov
networks. Clearly, in a trivial sense, they are equivalent (clique of
entire graph), but I presume some probability distributions are more
naturally represented as Bayes nets and others with Markov networks.

**Sampling**

Ancestral sampling of Bayes nets is very efficient. If you need to
condition on something upstream and then sample on something
downstream, that is cool too. But if you need to sample from something
upstream, then you will no longer have a fast sampling approach.

Of course, you can do Gibbs sampling on undirected models, but you
have no real assurance of when this will mix.

An advantage to structured models is that we can inject our knowledge
by choosing a structure we think is appropriate. For instance,
consider LDA.

**Hidden Variables**

These can often be added and then relationships between the visible
units can be expressed more succinctly. And the hidden units now
become a useful encoding/representation that can be used for other
tasks.

**Inference**

Can be hard to calculate probabilities. Examples when we want to do
this: expected hidden value code for visible units, conditional
visible unit query (`p(y | x)`; `y` isn't hidden per se, because it is
observed in the training set), maximum likelihood training (because we
need `p(x)`).

Most models we use won't allow exact inference. But approximate
inference can be fast. Most common DL approximate inference technique
is variational inference.

**Deep Graphical Models**

They mention that the big distinction is that latent variables have no
explicitly intended purpose in Deep Models. We figure that the model
will learn that.

Deep models also have dense connectivity to latent variables. Thus
Loopy Belief Propagation, a typical approximation approach in sparse
networks, doesn't work well in these dense networks. So a big
difference from classic PGM is that we don't use LBP.

**Restricted Boltzmann Machine**

The quintessential deep undirected model. Single layer of hidden units
and of visible units. Only pairwise connections between hidden and
visible units. Dense connectivity. Designed to be easy to perform
Gibbs sampling on.

RBM has factorial conditional distributions `p(v | h)` and `p(h |
v)`. The first is `\prod_i p(v_i | h)` and the second is `\prod_i
p(h_i | v)`.

Likewise, `p(v_i = 1 | h)` is easy to compute. We take `\sigma(b_i +
W_{i, :} h)`.

This makes it very easy to do *block Gibbs sampling*. So sampling can
be fast!

It is easy to calculate the derivative of the energy function: if you
know both `v` and `h`. That means it is possible to do maximum
likelihood if everything were visible. But I assume we have an EM part
here, where we use the derivatives for optimization in the inner loop?
