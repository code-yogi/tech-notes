## Week 1: Lecture 1

* Suggests that two main takeaways from brain:
    * Can learn from experience (reinforcement).
    * Can learn from deductions about a model of the world (logic).
    * Claims that lack of complete database of all facts (or the
      ability to create this), means that systems will unavoidably
      have to deal with uncertainty.
    * Gives chess playing as an example where computers don't seem to
      think like humans, but are successful nonetheless.

## Week 2: Lecture 2 + Lecture 3

* Planning ahead by searching.
    * Look through sequence of actions.
    * Don't need to enumerate all action sequences, the agent will
      search for the best one in this context.
* DFS, BFS, IDDFS
* Dijkstra
* "Greedy" search. DFS based on heuristic.
* A-star.
    * Discussed admissability of a heuristic.
    * Used not just for path-finding, but also translation/parsing.
    * I guess any "sequential" process, like language.
* Coming up with heuristic through relaxation.
    * Tiles out of place.
    * Distance from tiles to be in place.
    * Taking max of heuristics.
    * Subproblems.
* Avoid repeated positions.
    * This means keeping a list of places you have been before.
    * That will tend to take more memory.
    * But prolly not a lot more, if you're doing A-star...
* Need to be careful. Got to make sure the cost of a path *keeps going
  up*.
    * Otherwise we might get to an intermediate node "the wrong way".
    * This property is *consistency*.
    * Which is the same as saying that the cost of an arc, plus the
      heuristic cost from the next vertex, most always be *greater*
      than the hueristic cost of this node.
    * Basically, we can never become more enthusiastic about a path.
* Might use an inadmissable heuristic if it is:
    * Faster to compute
    * Better approximates true cost, even if it overestimates
      sometimes.

## Week3: Lecture 4 + Lecture 5

* CSP
* Different formulations for same problem:
    * N-queens; could have each position be a binary variable.
    * Or could have a variable for each row, and the position of the
      queen in this row.
    * Probably prefer higher cardinality, but lower number of
      variables.
    * Constraint graph when variables are involved in pairwise
      constraint.
* With multi-way constraints, we can draw this in the graph as a
  different vertex type, connecting all vertices involved.
* Interesting use:
    * Interpretation of line drawing of a 3-d scene.
    * Try to reconstruct the 3d world, but need to assign to each
      corner of the drawing it's position relative to the others.
* Real-world:
    * Assignment, scheduling, circuit layout
* CSP can be seen as search, where each action is to assign a
  variable.
    * Goal is everyone assigned and consistent.
* BFS is not going to be great, because all successful assignments are
  at same depth.
* DFS, but want to stop early.
    * Stop as early as an assignment is inconsistent with constraints.
    * This is backtracking.
* Forward-checking
    * This makes sure that as soon as you can't assign to any
      variable, you will backtrack.
    * Can take that further with "arc-consistency", which propegates
      effects.
        * To do this, remember the AC-3 or whatever algorithm. Note
          that everytime we eliminate a possibility in the domain,
          then we have to recheck all neighbors.
* Order:
    * Pick variables by least possible assignments.
    * Pick values by least possibilities eliminated.
    * These plus arc-consistency let you do 1k queens.
* Can have different levels of consistency (k-consistency)
    * But that costs more, and has less of an impact.
    * Typically we just stick with 2- or 3-consistency.
* Obviously if we can break problem down into parts, that's a big win.
    * One useful structure is tree structure.
    * In that case work at the leaves and work up, propagating
      constraints.
    * Then work your way down assigning values.
    * Basically: every parent value is compatible with *some* child
      value. So start picking values for the top, and you'll always
      have a choice in the child.
* By removing triangulating nodes, we can reduce to a tree. So if we
  can find those nodes (called a *cycle cutset*), we can assign it a
  value, then we're left with a tree, which is an easy problem to
  solve now.
    * You'll have to repeat this process for many assignments to the
      cutset variables. That's sort of like an "outer-loop" to this
      technique.
    * But finding the cutset is NP-hard.
    * But there are approximation methods that tend to work well.
* Local search methods
    * Hill-climbing, iterative improvement.
        * Pick the change that reduces # of violated constraints the
          most.
        * Could have randomness by randomly picking a variable and
          picking the minconflicts value. This is
          "iterative-improvement".
            * I guess that would be faster to step forward?
            * Otherwise I don't see a reason to choose randomly.
            * Could pick the variable which has the best change in
              value.
            * But that requires looking at all variables.
    * It appears that most randomly generated CSPs have solutions that
      are quickly found with iterative improvement.
        * But in non-random CSPs, this doesn't necessarily work.
    * Hill climbing can make sense in contexts where any solution will
      be acceptable, but you want the best you can reasonably come up
      with. Basically: you don't need optimality.
    * Random restarts.
    * Simulated annealing.
        * You can show that, as you iterate simulated annealing for
          longer, the time you spend at a location is proportionate to
          its fitness.
        * But that's not very helpful if you have to run this for
          insanely long periods of time.
    * Genetic algorithms.
        * Selection forces you up hill.
        * Point mutation is randomness to get stuck out of local
          optima.
        * Cross-breeding possibly gets you stuck out of big plateau.
            * I think this is what's called a "ridge-operator".
        * Got to be really careful of your representation; could a
          part of one solution ever make sense with part of another
          solution?
    * Tabu search could help avoid getting stuck in cycles.
        * Basic tabu search just bans returning to recent attempts.
        * But tabu search can also be about biasing toward promising,
          unexplored areas of the search space.
        * Seems like tabu search is really a family of
          algorithms/heuristics...
* Conflict-directed backjumping
    * They don't talk about it!

## Week 4

* Adversarial search.
* Minimax search.
    * Talks about evaluation function.
    * Obviously is just trying to predict true value of this node
      under minimax.
    * Evaluation function is useful only insofar as it correctly
      represents value of the node.
    * E.g., chess piece valuation is probably kinda weak.
* Alpha-Beta
* Expectimax
    * Lots of situations where you face uncertainty, but not an
      "adversary".
    * Randomness can come from lack of information, or insufficient
      model of the world, not just random events.
    * Basically, we'll replace min with expectation.
    * Mentions that you can't go too deep in this tree.
    * Heuristic for values needs to be more accurate, because you're
      taking expectations. Decision of which choice to make is more
      sensitive.
    * Probabilities of events: where do they come from? Presumably
      from some ML model.
* Let's say you want to do expectimax. Then you need to assign
  probabilities to each move they might make. This can be very slow,
  especially if they have a model of what *you* would do!
* Argues that expectimax reasoning is necessarty in most situations,
  because there are always very unlikely worst case possibilities.
* Argues that ideally we can just figure out utilities and then run a
  procedure that will figure out the correct behavior to maximize
  expected utility.
* Can prove that basic laws of rationality imply a utility function
  where you want to maximize expected utility.
    * If you are risk averse, and would like to avoid variance, you
      typically can't use just expected utility.
    * But what they're saying is that there is *some* utility function
      that captures your preferences.
